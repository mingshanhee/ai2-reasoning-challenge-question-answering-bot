{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QA bot try try.ipynb","provenance":[],"authorship_tag":"ABX9TyNzIE6vPqGIINn4lxo6xSPa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kciH3NztJWxX","colab_type":"code","outputId":"28110137-f4ea-43cf-a54e-b612b3551d3d","executionInfo":{"status":"ok","timestamp":1585197174732,"user_tz":-480,"elapsed":23403,"user":{"displayName":"Su Yee Liew","photoUrl":"","userId":"02829138366382988199"}},"colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TZytYfFkJYXB","colab_type":"code","colab":{}},"source":["from nltk.corpus import stopwords, wordnet\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk import pos_tag,word_tokenize,ne_chunk\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tree import Tree\n","from nltk import pos_tag,ne_chunk\n","# from DateExtractor import extractDate\n","import json\n","import math\n","import re\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QneuuedxMwkE","colab_type":"code","outputId":"517dd60d-1a9e-4957-ff23-821b1aaf2d87","executionInfo":{"status":"ok","timestamp":1585197251059,"user_tz":-480,"elapsed":59645,"user":{"displayName":"Su Yee Liew","photoUrl":"","userId":"02829138366382988199"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import nltk\n","nltk.download('all')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"h-PW1B8KK04R","colab_type":"code","colab":{}},"source":["# Code for tagging temporal expressions in text\n","# For details of the TIMEX format, see http://timex2.mitre.org/\n","\n","import re\n","import string\n","\n","# Requires eGenix.com mx Base Distribution\n","# http://www.egenix.com/products/python/mxBase/\n","\n","# Predefined strings.\n","numbers = \"(^a(?=\\s)|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand)\"\n","day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n","week_day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n","month = \"(january|february|march|april|may|june|july|august|september|october|november|december)\"\n","dmy = \"(year|day|week|month)\"\n","rel_day = \"(today|yesterday|tomorrow|tonight|tonite)\"\n","exp1 = \"(before|after|earlier|later|ago)\"\n","exp2 = \"(this|next|last)\"\n","iso = \"\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+\"\n","year = \"((?<=\\s)\\d{4}|^\\d{4})\"\n","regxp1 = \"((\\d+|(\" + numbers + \"[-\\s]?)+) \" + dmy + \"s? \" + exp1 + \")\"\n","regxp2 = \"(\" + exp2 + \" (\" + dmy + \"|\" + week_day + \"|\" + month + \"))\"\n","\n","date = \"([012]?[0-9]|3[01])\"\n","regxp3 = \"(\" + date + \" \" + month + \" \" + year + \")\"\n","regxp4 = \"(\" + month + \" \" + date + \"[th|st|rd]?[,]? \" + year + \")\"\n","\n","reg1 = re.compile(regxp1, re.IGNORECASE)\n","reg2 = re.compile(regxp2, re.IGNORECASE)\n","reg3 = re.compile(rel_day, re.IGNORECASE)\n","reg4 = re.compile(iso)\n","reg5 = re.compile(year)\n","reg6 = re.compile(regxp3, re.IGNORECASE)\n","reg7 = re.compile(regxp4, re.IGNORECASE)\n","\n","def extractDate(text):\n","\n","    # Initialization\n","    timex_found = []\n","\n","    # re.findall() finds all the substring matches, keep only the full\n","    # matching string. Captures expressions such as 'number of days' ago, etc.\n","    found = reg1.findall(text)\n","    found = [a[0] for a in found if len(a) > 1]\n","    for timex in found:\n","        timex_found.append(timex)\n","\n","    # Variations of this thursday, next year, etc\n","    found = reg2.findall(text)\n","    found = [a[0] for a in found if len(a) > 1]\n","    for timex in found:\n","        timex_found.append(timex)\n","\n","    # today, tomorrow, etc\n","    found = reg3.findall(text)\n","    for timex in found:\n","        timex_found.append(timex)\n","\n","    # ISO\n","    found = reg4.findall(text)\n","    for timex in found:\n","        timex_found.append(timex)\n","\n","    # Dates\n","    found = reg6.findall(text)\n","    found = [a[0] for a in found if len(a) > 1]\n","    for timex in found:\n","        timex_found.append(timex)\n","\n","    found = reg7.findall(text)\n","    found = [a[0] for a in found if len(a) > 1]\n","    for timex in found:\n","        timex_found.append(timex)\n","\n","    # Year\n","    found = reg5.findall(text)\n","    for timex in found:\n","        timex_found.append(timex)\n","    # Tag only temporal expressions which haven't been tagged.\n","    #for timex in timex_found:\n","    #    text = re.sub(timex + '(?!</TIMEX2>)', '<TIMEX2>' + timex + '</TIMEX2>', text)\n","\n","    return timex_found"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vwdRJVo9KIjC","colab_type":"text"},"source":["##ProcessedQuestion"]},{"cell_type":"code","metadata":{"id":"O4ZigFyvJreV","colab_type":"code","colab":{}},"source":["# ScriptName : ProcessedQuestion.py\n","# Description : Takes question as an input and process it to find out question\n","#   and answer type, also prepare question vector and prepare search query for\n","#   Information Retrieval process\n","# Arguments : \n","#       Input :\n","#           question(str) : String of question\n","#           useStemmer(boolean) : Indicate to use stemmer for question tokens\n","#           useSynonyms(boolean) : Indicate to use thesaraus for query expansion\n","#           removeStopwords(boolean) : Indicate to remove stop words from search\n","#                                      query\n","#       Output :\n","#           Instance of ProcessedQuestion with useful following structure\n","#               qVector(dict) : Key Value pair of word and its frequency\n","#                               to be used for Information Retrieval and \n","#                               similarity calculation\n","#               question(str) : Raw question\n","#               qType(str) : Type of question\n","#               aType(str) : Expected answer type\n","#                       [\"PERSON\",\"LOCATION\",\"DATE\",\"DEFINITION\",\"YESNO\"]\n","#  \n","\n","class ProcessedQuestion:\n","    def __init__(self, question, useStemmer = False, useSynonyms = False, removeStopwords = False):\n","        self.question = question\n","        self.useStemmer = useStemmer\n","        self.useSynonyms = useSynonyms\n","        self.removeStopwords = removeStopwords\n","        self.stopWords = stopwords.words(\"english\")\n","        self.stem = lambda k : k.lower()\n","        if self.useStemmer:\n","            ps = PorterStemmer()\n","            self.stem = ps.stem\n","        self.qType = self.determineQuestionType(question)\n","        self.searchQuery = self.buildSearchQuery(question)\n","        self.qVector = self.getQueryVector(self.searchQuery)\n","        self.aType = self.determineAnswerType(question)\n","    \n","    # To determine type of question by analyzing POS tag of question from Penn \n","    # Treebank tagset\n","    #\n","    # Input:\n","    #           question(str) : Question string\n","    # Output:\n","    #           qType(str) : Type of question among following\n","    #                   [ WP ->  who\n","    #                     WDT -> what, why, how\n","    #                     WP$ -> whose\n","    #                     WRB -> where ]\n","    def determineQuestionType(self, question):\n","        questionTaggers = ['WP','WDT','WP$','WRB']\n","        qPOS = pos_tag(word_tokenize(question))\n","        qTags = []\n","        for token in qPOS:\n","            if token[1] in questionTaggers:\n","                qTags.append(token[1])\n","        qType = ''\n","        if(len(qTags)>1):\n","            qType = 'complex'\n","        elif(len(qTags) == 1):\n","            qType = qTags[0]\n","        else:\n","            qType = \"None\"\n","        return qType\n","    \n","    # To determine type of expected answer depending of question type\n","    #\n","    # Input:\n","    #           question(str) : Question string\n","    # Output:\n","    #           aType(str) : Type of answer among following\n","    #               [PERSON, LOCATION, DATE, ORGANIZATION, QUANTITY, DEFINITION\n","    #                   FULL]\n","    def determineAnswerType(self, question):\n","        questionTaggers = ['WP','WDT','WP$','WRB']\n","        qPOS = pos_tag(word_tokenize(question))\n","        qTag = None\n","\n","        for token in qPOS:\n","            if token[1] in questionTaggers:\n","                qTag = token[0].lower()\n","                break\n","        \n","        if(qTag == None):\n","            if len(qPOS) > 1:\n","                if qPOS[1][1].lower() in ['is','are','can','should']:\n","                    qTag = \"YESNO\"\n","        #who/where/what/why/when/is/are/can/should\n","        if qTag == \"who\":\n","            return \"PERSON\"\n","        elif qTag == \"where\":\n","            return \"LOCATION\"\n","        elif qTag == \"when\":\n","            return \"DATE\"\n","        elif qTag == \"what\":\n","            # Defination type question\n","            # If question of type whd modal noun? its a defination question\n","            qTok = self.getContinuousChunk(question)\n","            #print(qTok)\n","            if(len(qTok) == 4):\n","                if qTok[1][1] in ['is','are','was','were'] and qTok[2][0] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]:\n","                    self.question = \" \".join([qTok[0][1],qTok[2][1],qTok[1][1]])\n","                    #print(\"Type of question\",\"Definition\",self.question)\n","                    return \"DEFINITION\"\n","\n","            # ELSE USE FIRST HEAD WORD\n","            for token in qPOS:\n","                if token[0].lower() in [\"city\",\"place\",\"country\"]:\n","                    return \"LOCATION\"\n","                elif token[0].lower() in [\"company\",\"industry\",\"organization\"]:\n","                    return \"ORGANIZATION\"\n","                elif token[1] in [\"NN\",\"NNS\"]:\n","                    return \"FULL\"\n","                elif token[1] in [\"NNP\",\"NNPS\"]:\n","                    return \"FULL\"\n","            return \"FULL\"\n","        elif qTag == \"how\":\n","            if len(qPOS)>1:\n","                t2 = qPOS[2]\n","                if t2[0].lower() in [\"few\",\"great\",\"little\",\"many\",\"much\"]:\n","                    return \"QUANTITY\"\n","                elif t2[0].lower() in [\"tall\",\"wide\",\"big\",\"far\"]:\n","                    return \"LINEAR_MEASURE\"\n","            return \"FULL\"\n","        else:\n","            return \"FULL\"\n","    \n","    # To build search query by dropping question word\n","    #\n","    # Input:\n","    #           question(str) : Question string\n","    # Output:\n","    #           searchQuery(list) : List of tokens\n","    def buildSearchQuery(self, question):\n","        qPOS = pos_tag(word_tokenize(question))\n","        searchQuery = []\n","        questionTaggers = ['WP','WDT','WP$','WRB']\n","        for tag in qPOS:\n","            if tag[1] in questionTaggers:\n","                continue\n","            else:\n","                searchQuery.append(tag[0])\n","                if(self.useSynonyms):\n","                    syn = self.getSynonyms(tag[0])\n","                    if(len(syn) > 0):\n","                        searchQuery.extend(syn)\n","        return searchQuery\n","    \n","    # To build query vector\n","    #\n","    # Input:\n","    #       searchQuery(list) : List of tokens from buildSearchQuery method\n","    # Output:\n","    #       qVector(dict) : Dictionary of words and their frequency\n","    def getQueryVector(self, searchQuery):\n","        vector = {}\n","        for token in searchQuery:\n","            if self.removeStopwords:\n","                if token in self.stopWords:\n","                    continue\n","            token = self.stem(token)\n","            if token in vector.keys():\n","                vector[token] += 1\n","            else:\n","                vector[token] = 1\n","        return vector\n","    \n","    # To get continuous chunk of similar POS tags.\n","    # E.g.  If two NN tags are consequetive, this method will merge and return\n","    #       single NN with combined value.\n","    #       It is helpful in detecting name of single person like John Cena, \n","    #       Steve Jobs\n","    # Input:\n","    #       question(str) : question string\n","    # Output:\n","    #       \n","    def getContinuousChunk(self,question):\n","        chunks = []\n","        answerToken = word_tokenize(question)\n","        nc = pos_tag(answerToken)\n","\n","        prevPos = nc[0][1]\n","        entity = {\"pos\":prevPos,\"chunk\":[]}\n","        for c_node in nc:\n","            (token,pos) = c_node\n","            if pos == prevPos:\n","                prevPos = pos       \n","                entity[\"chunk\"].append(token)\n","            elif prevPos in [\"DT\",\"JJ\"]:\n","                prevPos = pos\n","                entity[\"pos\"] = pos\n","                entity[\"chunk\"].append(token)\n","            else:\n","                if not len(entity[\"chunk\"]) == 0:\n","                    chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n","                    entity = {\"pos\":pos,\"chunk\":[token]}\n","                    prevPos = pos\n","        if not len(entity[\"chunk\"]) == 0:\n","            chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n","        return chunks\n","    \n","    # To get synonyms of word in order to improve query by using query\n","    # expanision technique\n","    # Input:\n","    #       word(str) : Word token\n","    # Output:\n","    #       synonyms(list) : List of synonyms of given word\n","    def getSynonyms(word):\n","        synonyms = []\n","        for syn in wordnet.synsets(word):\n","            for l in syn.lemmas():\n","                w = l.name().lower()\n","                synonyms.extend(w.split(\"_\"))\n","        return list(set(synonyms))\n","    \n","    # String representation of this class\n","    def __repr__(self):\n","        msg = \"Q: \" + self.question + \"\\n\"\n","        msg += \"QType: \" + self.qType + \"\\n\"\n","        msg += \"QVector: \" + str(self.qVector) + \"\\n\"\n","        return msg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAqnIS2kKZNj","colab_type":"text"},"source":["##DocumentRetrievalModel"]},{"cell_type":"code","metadata":{"id":"tIOgSn00KXWG","colab_type":"code","colab":{}},"source":["# ScriptName : DocumentRetrievalModel.py\n","# Description : Script preprocesses article and paragraph to computer TFIDF.\n","#               Additionally, helps in answer processing \n","# Arguments : \n","#       Input :\n","#           paragraphs(list)        : List of paragraphs\n","#           useStemmer(boolean)     : Indicate to use stemmer for word tokens\n","#           removeStopWord(boolean) : Indicate to remove stop words from \n","#                                     paragraph in order to keep relevant words\n","#       Output :\n","#           Instance of DocumentRetrievalModel with following structure\n","#               query(function) : Take instance of processedQuestion and return\n","#                                 answer based on IR and Answer Processing\n","#                                 techniques\n","\n","class DocumentRetrievalModel:\n","    def __init__(self,paragraphs,removeStopWord = False,useStemmer = False):\n","        self.idf = {}               # dict to store IDF for words in paragraph\n","        self.paragraphInfo = {}     # structure to store paragraphVector\n","        self.paragraphs = paragraphs\n","        self.totalParas = len(paragraphs)\n","        self.stopwords = stopwords.words('english')\n","        self.removeStopWord = removeStopWord\n","        self.useStemmer = useStemmer\n","        self.vData = None\n","        self.stem = lambda k:k.lower()\n","        if(useStemmer):\n","            ps = PorterStemmer()\n","            self.stem = ps.stem\n","            \n","        # Initialize\n","        self.computeTFIDF()\n","        \n","    # Return term frequency for Paragraph\n","    # Input:\n","    #       paragraph(str): Paragraph as a whole in string format\n","    # Output:\n","    #       wordFrequence(dict) : Dictionary of word and term frequency\n","    def getTermFrequencyCount(self,paragraph):\n","        sentences = sent_tokenize(paragraph)\n","        wordFrequency = {}\n","        for sent in sentences:\n","            for word in word_tokenize(sent):\n","                if self.removeStopWord == True:\n","                    if word.lower() in self.stopwords:\n","                        #Ignore stopwords\n","                        continue\n","                    if not re.match(r\"[a-zA-Z0-9\\-\\_\\\\/\\.\\']+\",word):\n","                        continue\n","                #Use of Stemmer\n","                if self.useStemmer:\n","                    word = self.stem(word)\n","                    \n","                if word in wordFrequency.keys():\n","                    wordFrequency[word] += 1\n","                else:\n","                    wordFrequency[word] = 1\n","        return wordFrequency\n","    \n","    # Computes term-frequency inverse document frequency for every token of each\n","    # paragraph\n","    # Output:\n","    #       paragraphInfo(dict): Dictionary for every paragraph with following \n","    #                            keys\n","    #                               vector : dictionary of TFIDF for every word\n","    def computeTFIDF(self):\n","        # Compute Term Frequency\n","        self.paragraphInfo = {}\n","        for index in range(0,len(self.paragraphs)):\n","            wordFrequency = self.getTermFrequencyCount(self.paragraphs[index])\n","            self.paragraphInfo[index] = {}\n","            self.paragraphInfo[index]['wF'] = wordFrequency\n","        \n","        wordParagraphFrequency = {}\n","        for index in range(0,len(self.paragraphInfo)):\n","            for word in self.paragraphInfo[index]['wF'].keys():\n","                if word in wordParagraphFrequency.keys():\n","                    wordParagraphFrequency[word] += 1\n","                else:\n","                    wordParagraphFrequency[word] = 1\n","        \n","        self.idf = {}\n","        for word in wordParagraphFrequency:\n","            # Adding Laplace smoothing by adding 1 to total number of documents\n","            self.idf[word] = math.log((self.totalParas+1)/wordParagraphFrequency[word])\n","        \n","        #Compute Paragraph Vector\n","        for index in range(0,len(self.paragraphInfo)):\n","            self.paragraphInfo[index]['vector'] = {}\n","            for word in self.paragraphInfo[index]['wF'].keys():\n","                self.paragraphInfo[index]['vector'][word] = self.paragraphInfo[index]['wF'][word] * self.idf[word]\n","    \n","\n","    # To find answer to the question by first finding relevant paragraph, then\n","    # by finding relevant sentence and then by procssing sentence to get answer\n","    # based on expected answer type\n","    # Input:\n","    #           pQ(ProcessedQuestion) : Instance of ProcessedQuestion\n","    # Output:\n","    #           answer(str) : Response of QA System\n","    def query(self,pQ):\n","        \n","        # Get relevant Paragraph\n","        relevantParagraph = self.getSimilarParagraph(pQ.qVector)\n","\n","        # Get All sentences\n","        sentences = []\n","        for tup in relevantParagraph:\n","            if tup != None:\n","                p2 = self.paragraphs[tup[0]]\n","                sentences.extend(sent_tokenize(p2))\n","        \n","        # Get Relevant Sentences\n","        if len(sentences) == 0:\n","            return \"Oops! Unable to find answer\"\n","\n","        # Get most relevant sentence using unigram similarity\n","        relevantSentences = self.getMostRelevantSentences(sentences,pQ,1)\n","\n","        # AnswerType\n","        aType = pQ.aType\n","        \n","        # Default Answer\n","        answer = relevantSentences[0][0]\n","\n","        ps = PorterStemmer()\n","        # For question type looking for Person\n","        if aType == \"PERSON\":\n","            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n","            for entity in ne:\n","                if entity[0] == \"PERSON\":\n","                    answer = entity[1]\n","                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n","                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n","                    # If any entity is already in question\n","                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n","                        continue\n","                    break\n","        elif aType == \"LOCATION\":\n","            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n","            for entity in ne:\n","                if entity[0] == \"GPE\":\n","                    answer = entity[1]\n","                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n","                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n","                    # If any entity is already in question\n","                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n","                        continue\n","                    break\n","        elif aType == \"ORGANIZATION\":\n","            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n","            for entity in ne:\n","                if entity[0] == \"ORGANIZATION\":\n","                    answer = entity[1]\n","                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n","                    # If any entity is already in question\n","                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n","                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n","                        continue\n","                    break\n","        elif aType == \"DATE\":\n","            allDates = []\n","            for s in relevantSentences:\n","                allDates.extend(extractDate(s[0]))\n","            if len(allDates)>0:\n","                answer = allDates[0]\n","        elif aType in [\"NN\",\"NNP\"]:\n","            candidateAnswers = []\n","            ne = self.getContinuousChunk([s[0] for s in relevantSentences])\n","            for entity in ne:\n","                if aType == \"NN\":\n","                    if entity[0] == \"NN\" or entity[0] == \"NNS\":\n","                        answer = entity[1]\n","                        answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n","                        qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n","                        # If any entity is already in question\n","                        if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n","                            continue\n","                        break\n","                elif aType == \"NNP\":\n","                    if entity[0] == \"NNP\" or entity[0] == \"NNPS\":\n","                        answer = entity[1]\n","                        answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n","                        qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n","                        # If any entity is already in question\n","                        if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n","                            continue\n","                        break\n","        elif aType == \"DEFINITION\":\n","            relevantSentences = self.getMostRelevantSentences(sentences,pQ,1)\n","            answer = relevantSentences[0][0]\n","        return answer\n","        \n","    # Get top 3 relevant paragraph based on cosine similarity between question \n","    # vector and paragraph vector\n","    # Input :\n","    #       queryVector(dict) : Dictionary of words in question with their \n","    #                           frequency\n","    # Output:\n","    #       pRanking(list) : List of tuple with top 3 paragraph with its\n","    #                        similarity coefficient\n","    def getSimilarParagraph(self,queryVector):    \n","        queryVectorDistance = 0\n","        for word in queryVector.keys():\n","            if word in self.idf.keys():\n","                queryVectorDistance += math.pow(queryVector[word]*self.idf[word],2)\n","        queryVectorDistance = math.pow(queryVectorDistance,0.5)\n","        if queryVectorDistance == 0:\n","            return [None]\n","        pRanking = []\n","        for index in range(0,len(self.paragraphInfo)):\n","            sim = self.computeSimilarity(self.paragraphInfo[index], queryVector, queryVectorDistance)\n","            pRanking.append((index,sim))\n","        \n","        return sorted(pRanking,key=lambda tup: (tup[1],tup[0]), reverse=True)[:3]\n","    \n","    # Compute cosine similarity betweent queryVector and paragraphVector\n","    # Input:\n","    #       pInfo(dict)         : Dictionary containing wordFrequency and \n","    #                             paragraph Vector\n","    #       queryVector(dict)   : Query vector for question\n","    #       queryDistance(float): Distance of queryVector from origin\n","    # Output:\n","    #       sim(float)          : Cosine similarity coefficient\n","    def computeSimilarity(self, pInfo, queryVector, queryDistance):\n","        # Computing pVectorDistance\n","        pVectorDistance = 0\n","        for word in pInfo['wF'].keys():\n","            pVectorDistance += math.pow(pInfo['wF'][word]*self.idf[word],2)\n","        pVectorDistance = math.pow(pVectorDistance,0.5)\n","        if(pVectorDistance == 0):\n","            return 0\n","\n","        # Computing dot product\n","        dotProduct = 0\n","        for word in queryVector.keys():\n","            if word in pInfo['wF']:\n","                q = queryVector[word]\n","                w = pInfo['wF'][word]\n","                idf = self.idf[word]\n","                dotProduct += q*w*idf*idf\n","        \n","        sim = dotProduct / (pVectorDistance * queryDistance)\n","        return sim\n","    \n","    # Get most relevant sentences using unigram similarity between question\n","    # sentence and sentence in paragraph containing potential answer\n","    # Input:\n","    #       sentences(list)      : List of sentences in order of occurance as in\n","    #                              paragraph\n","    #       pQ(ProcessedQuestion): Instance of processedQuestion\n","    #       nGram(int)           : Value of nGram (default 3)\n","    # Output:\n","    #       relevantSentences(list) : List of tuple with sentence and their\n","    #                                 similarity coefficient\n","    def getMostRelevantSentences(self, sentences, pQ, nGram=3):\n","        relevantSentences = []\n","        for sent in sentences:\n","            sim = 0\n","            if(len(word_tokenize(pQ.question))>nGram+1):\n","                sim = self.sim_ngram_sentence(pQ.question,sent,nGram)\n","            else:\n","                sim = self.sim_sentence(pQ.qVector, sent)\n","            relevantSentences.append((sent,sim))\n","        \n","        return sorted(relevantSentences,key=lambda tup:(tup[1],tup[0]),reverse=True)\n","    \n","    # Compute ngram similarity between a sentence and question\n","    # Input:\n","    #       question(str)   : Question string\n","    #       sentence(str)   : Sentence string\n","    #       nGram(int)      : Value of n in nGram\n","    # Output:\n","    #       sim(float)      : Ngram Similarity Coefficient\n","    def sim_ngram_sentence(self, question, sentence,nGram):\n","        #considering stop words as well\n","        ps = PorterStemmer()\n","        getToken = lambda question:[ ps.stem(w.lower()) for w in word_tokenize(question) ]\n","        getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n","        qToken = getToken(question)\n","        sToken = getToken(sentence)\n","\n","        if(len(qToken) > nGram):\n","            q3gram = set(getNGram(qToken,nGram))\n","            s3gram = set(getNGram(sToken,nGram))\n","            if(len(s3gram) < nGram):\n","                return 0\n","            qLen = len(q3gram)\n","            sLen = len(s3gram)\n","            sim = len(q3gram.intersection(s3gram)) / len(q3gram.union(s3gram))\n","            return sim\n","        else:\n","            return 0\n","    \n","    # Compute similarity between sentence and queryVector based on number of \n","    # common words in both sentence. It doesn't consider occurance of words\n","    # Input:\n","    #       queryVector(dict)   : Dictionary of words in question\n","    #       sentence(str)       : Sentence string\n","    # Ouput:\n","    #       sim(float)          : Similarity Coefficient    \n","    def sim_sentence(self, queryVector, sentence):\n","        sentToken = word_tokenize(sentence)\n","        ps = PorterStemmer()\n","        for index in range(0,len(sentToken)):\n","            sentToken[index] = ps.stem(sentToken[index])\n","        sim = 0\n","        for word in queryVector.keys():\n","            w = ps.stem(word)\n","            if w in sentToken:\n","                sim += 1\n","        return sim/(len(sentToken)*len(queryVector.keys()))\n","    \n","    # Get Named Entity from the sentence in form of PERSON, GPE, & ORGANIZATION\n","    # Input:\n","    #       answers(list)       : List of potential sentence containing answer\n","    # Output:\n","    #       chunks(list)        : List of tuple with entity and name in ranked \n","    #                             order\n","    def getNamedEntity(self,answers):\n","        chunks = []\n","        for answer in answers:\n","            answerToken = word_tokenize(answer)\n","            nc = ne_chunk(pos_tag(answerToken))\n","            entity = {\"label\":None,\"chunk\":[]}\n","            for c_node in nc:\n","                if(type(c_node) == Tree):\n","                    if(entity[\"label\"] == None):\n","                        entity[\"label\"] = c_node.label()\n","                    entity[\"chunk\"].extend([ token for (token,pos) in c_node.leaves()])\n","                else:\n","                    (token,pos) = c_node\n","                    if pos == \"NNP\":\n","                        entity[\"chunk\"].append(token)\n","                    else:\n","                        if not len(entity[\"chunk\"]) == 0:\n","                            chunks.append((entity[\"label\"],\" \".join(entity[\"chunk\"])))\n","                            entity = {\"label\":None,\"chunk\":[]}\n","            if not len(entity[\"chunk\"]) == 0:\n","                chunks.append((entity[\"label\"],\" \".join(entity[\"chunk\"])))\n","        return chunks\n","    \n","    # To get continuous chunk of similar POS tags.\n","    # E.g.  If two NN tags are consequetive, this method will merge and return\n","    #       single NN with combined value.\n","    #       It is helpful in detecting name of single person like John Cena, \n","    #       Steve Jobs\n","    # Input:\n","    #       answers(list) : list of potential sentence string\n","    # Output:\n","    #       chunks(list)  : list of tuple with entity and name in ranked order\n","    def getContinuousChunk(self,answers):\n","        chunks = []\n","        for answer in answers:\n","            answerToken = word_tokenize(answer)\n","            if(len(answerToken)==0):\n","                continue\n","            nc = pos_tag(answerToken)\n","            \n","            prevPos = nc[0][1]\n","            entity = {\"pos\":prevPos,\"chunk\":[]}\n","            for c_node in nc:\n","                (token,pos) = c_node\n","                if pos == prevPos:\n","                    prevPos = pos       \n","                    entity[\"chunk\"].append(token)\n","                elif prevPos in [\"DT\",\"JJ\"]:\n","                    prevPos = pos\n","                    entity[\"pos\"] = pos\n","                    entity[\"chunk\"].append(token)\n","                else:\n","                    if not len(entity[\"chunk\"]) == 0:\n","                        chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n","                        entity = {\"pos\":pos,\"chunk\":[token]}\n","                        prevPos = pos\n","            if not len(entity[\"chunk\"]) == 0:\n","                chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n","        return chunks\n","    \n","    def getqRev(self, pq):\n","        if self.vData == None:\n","            # For testing purpose\n","            self.vData = json.loads(open(\"validatedata.py\",\"r\").readline())\n","        revMatrix = []\n","        for t in self.vData:\n","            sent = t[\"q\"]\n","            revMatrix.append((t[\"a\"],self.sim_sentence(pq.qVector,sent)))\n","        return sorted(revMatrix,key=lambda tup:(tup[1],tup[0]),reverse=True)[0][0]\n","        \n","    def __repr__(self):\n","        msg = \"Total Paras \" + str(self.totalParas) + \"\\n\"\n","        msg += \"Total Unique Word \" + str(len(self.idf)) + \"\\n\"\n","        msg += str(self.getMostSignificantWords())\n","        return msg"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nnjJbgiDLJzD","colab_type":"code","colab":{}},"source":["def sim_ngram_sentence(question, sentence,nGram=3):\n","    #considering stop words as well\n","    ps = PorterStemmer()\n","    getToken = lambda question:[ ps.stem(w.lower()) for w in word_tokenize(question) ]\n","    getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n","    qToken = getToken(question)\n","    sToken = getToken(sentence)\n","\n","    if(len(qToken) > nGram):\n","        q3gram = set(getNGram(qToken,nGram))\n","        s3gram = set(getNGram(sToken,nGram))\n","        if(len(s3gram) < nGram):\n","            return 0\n","        qLen = len(q3gram)\n","        sLen = len(s3gram)\n","        sim = len(q3gram.intersection(s3gram)) / len(q3gram.union(s3gram))\n","        return sim\n","    else:\n","        return 0\n","\n","def get_option(response, options):\n","    answers = []\n","    for opt in options:\n","        sim = sim_ngram_sentence(response, opt)\n","        answers.append((opt, sim))\n","\n","    answer = sorted(answers,key=lambda tup: (tup[1],tup[0]), reverse=True)[0][0]\n","    # answers_sim = sorted(answers,key=lambda tup: (tup[1],tup[0]), reverse=True)\n","    return answer\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMu0oTQjK8Pm","colab_type":"code","outputId":"2a91bc6a-0dab-4537-cbce-c812bbd7fd2e","executionInfo":{"status":"error","timestamp":1585197355938,"user_tz":-480,"elapsed":104744,"user":{"displayName":"Su Yee Liew","photoUrl":"","userId":"02829138366382988199"}},"colab":{"base_uri":"https://localhost:8080/","height":417}},"source":["print(\"Bot> Please wait, while I am loading my dependencies\")\n","# from DocumentRetrievalModel import DocumentRetrievalModel as DRM\n","# from ProcessedQuestion import ProcessedQuestion as PQ\n","import re\n","import sys\n","\n","# if len(sys.argv) == 1:\n","# \tprint(\"Bot> I need some reference to answer your question\")\n","# \tprint(\"Bot> Please! Rerun me using following syntax\")\n","# \tprint(\"\\t\\t$ python3 P2.py <datasetName>\")\n","# \tprint(\"Bot> You can find dataset name in \\\"dataset\\\" folder\")\n","# \tprint(\"Bot> Thanks! Bye\")\n","# \texit()\n","\n","# datasetName = sys.argv[1]\n","datasetName = '/content/drive/My Drive/TM self/Factoid-based-Question-Answer-Chatbot/testing3.txt' #change here to corpus txt with \n","# Loading Dataset\n","try:\n","\tdatasetFile = open(datasetName,\"r\")\n","except FileNotFoundError:\n","\tprint(\"Bot> Oops! I am unable to locate \\\"\" + datasetName + \"\\\"\")\n","\texit()\n","\n","# Retrieving paragraphs : Assumption is that each paragraph in dataset is\n","# separated by new line character\n","paragraphs = []\n","for para in datasetFile.readlines():\n","\tif(len(para.strip()) > 0):\n","\t\tparagraphs.append(para.strip())\n","\n","# Processing Paragraphs\n","drm = DocumentRetrievalModel(paragraphs,True,True) #docs, stem, stopw removal\n","\n","print(\"Bot> Hey! I am ready. Ask me factoid based questions only :P\")\n","print(\"Bot> You can say Bye anytime you want\")\n","\n","# Greet Pattern\n","greetPattern = re.compile(\"^\\ *((hi+)|((good\\ )?morning|evening|afternoon)|(he((llo)|y+)))\\ *$\",re.IGNORECASE)\n","\n","isActive = True\n","while isActive:\n","  userQuery = input(\"You> \")\n","  if not len(userQuery)>0:\n","    print(\"Bot> You need to ask something\")\n","  elif greetPattern.findall(userQuery):\n","    response = \"Hello!\"\n","  elif (userQuery.strip().lower() == \"bye\"):\n","    response = \"Bye Bye!\"\n","    isActive = False\n","  else:\n","    # split input into list of qns and options\n","    output = re.split(\"\\(\\w+\\)\", userQuery)\n","\n","    question = output[0]\n","    options = output[1:]\n","\n","\t\t# Proocess Question\n","    pq = ProcessedQuestion(question,True,False,True) #qns, stem, syn, stopw removal \n","\n","\t\t# Get Response From Bot\n","    answer = drm.query(pq)\n","\n","    # Get option by comparing with response\n","    response = get_option(answer, options)\n","\n","  print(\"Bot>\",response)\n","  print()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Bot> Please wait, while I am loading my dependencies\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-5f24f995ffce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Processing Paragraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdrm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentRetrievalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#docs, stem, stopw removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot> Hey! I am ready. Ask me factoid based questions only :P\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-9fa1232c0008>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, paragraphs, removeStopWord, useStemmer)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Initialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Return term frequency for Paragraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-9fa1232c0008>\u001b[0m in \u001b[0;36mcomputeTFIDF\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mwordFrequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetTermFrequencyCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparagraphInfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wF'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordFrequency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-9fa1232c0008>\u001b[0m in \u001b[0;36mgetTermFrequencyCount\u001b[0;34m(self, paragraph)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mwordFrequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremoveStopWord\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"tM6YYVu9TDk-","colab_type":"code","colab":{}},"source":["Which of the following is a typical example of a unicellular organism? (A) earthworm (B) bacteria (C) fungi (D) green algae\n","B\n","\n","What role does the centromere play in cellular reproduction? (A) It is the area where microtubules are formed. (B) It is the area where the nucleus is during cell division. (C) It is the area of alignment for the chromosomes. (D) It is the area of attachment for chromatids.\n","D\n","\n","What causes a blue block to appear blue in the sunlight? (A) The block absorbs all blue light. (B) The block bends (refracts) all blue light. (C) Only blue light is reflected by the block. (D) Only blue light passes through the block.\n","C\n","\n","To safely conduct an experiment using chemicals, what should students always do? (A) Work in large groups. (B) Wear safety goggles. (C) Wear short sleeves. (D) Keep a window open.\n","B\n","\n","Which are two parts of the carbon cycle? (A) freezing and thawing (B) growth and reproduction (C) evaporation and precipitation (D) photosynthesis and respiration\n","D\n","\n","The digestive system breaks food into simple substances that the body can use. What system carries these simple substances from the digestive system to other parts of the body? (A) circulatory (B) nervous (C) respiratory (D) skeletal\n","A\n","\n","What tool would be used to examine a fingerprint? (A) a graduated cylinder (B) a hand lens (C) a pair of goggles (D) a thermometer\n","B\n","\n","Which sense is used to tell if there is sugar in a glass of tea? (A) Touch (B) Hearing (C) Smell (D) Taste\n","D\n","\n","All organisms contain DNA and RNA. What are the subunits of DNA and RNA? (A) simple sugars (B) amino acids (C) carbohydrates (D) nucleotides\n","D\n","\n","What happens during photosynthesis? (A) Insects pollinate plants. (B) Plants change soil into food energy. (C) Animals get carbon dioxide from plants. (D) Plants change light energy into food energy.\n","D\n","\n","What is the role of decomposers in a food chain? (A) They consume other organisms. (B) They break down dead organic matter. (C) They use the Sun's energy to make food. (D) They convert inorganic matter into organic matter.\n","B\n","\n","Which part of the electromagnetic spectrum can humans sense without using equipment or technology? (A) radio waves (B) visible light (C) microwaves (D) X-rays\n","B\n","\n","What does a mirror do to light that causes objects to appear backwards? (A) refracts (B) reflects (C) absorbs (D) blocks\n","B"],"execution_count":0,"outputs":[]}]}