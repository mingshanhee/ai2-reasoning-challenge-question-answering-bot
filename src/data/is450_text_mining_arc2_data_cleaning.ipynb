{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPsPdXFrKesl"
   },
   "source": [
    "# 1. Mounting Google Drive\n",
    "Follow this guide (using last method, \"Bonus Method\"):\n",
    "https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92 \n",
    "\n",
    "Step 1: Add \"IS425 Text Mining Dataset\" folder from our shared folder into \"My Drive\" (right-click TM folder > \"add to My Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VIZU9DpEKmIn",
    "outputId": "38f81d7c-e6c4-4db3-b4fd-9874f8d355f4"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p8FEJwmbnlzP",
    "outputId": "bead9619-ba8a-40fa-8ee3-384375d702e7"
   },
   "outputs": [],
   "source": [
    "%cd \"drive/My Drive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkR2KdCpmh9_"
   },
   "source": [
    "# 2. Specifying Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9R2ksQqndE6o"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "\n",
    "from statistics import mean\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import time\n",
    "\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "# The following statement imports a class called PlaintextCorpusReader.\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "htyixzsLEEdj",
    "outputId": "20fb394e-41ee-4bd4-96e2-af5448e0f483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjqOr4kRj0qq"
   },
   "source": [
    "# 3. Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 RAW Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7C-8X-Bg_x2a"
   },
   "outputs": [],
   "source": [
    "# #Run if need access to corpus. Separated it because it takes a long time to run\n",
    "raw_folder = \"/content/drive/My Drive/IS 425 Text Mining Dataset/ARC/raw/\"\n",
    "interim_folder = \"/content/drive/My Drive/IS 425 Text Mining Dataset/ARC/interim/\"\n",
    "arc_corpus_filename = \"ARC_Corpus.txt\"\n",
    "\n",
    "# raw_folder = \"../../data/raw/\"\n",
    "# interim_folder = \"../../data/interim\"\n",
    "# arc_corpus_filename = \"ARC_Corpus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwTKpgFeQa2O"
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(raw_folder, arc_corpus_filename), \"r\")\n",
    "\n",
    "arc_lines = f.read().splitlines()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Splitting the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zc176cjQQ3fq",
    "outputId": "9fec38ee-0a93-495e-e2d7-fe2122ff1057"
   },
   "outputs": [],
   "source": [
    "n_cores = 60\n",
    "chunk_size = math.ceil(len(arc_lines) / n_cores)\n",
    "chunk_size = int(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "arc_lines_chunk =list(chunks(arc_lines, chunk_size))\n",
    "print(len(arc_lines_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large international companies are involved in bauxite, iron ore, diamond, and gold mining operations.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paleoceanography, 8(2): 193-208.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAkF08fEMY1q"
   },
   "source": [
    "# 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Expand Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction(lines):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "            \n",
    "        # replacing 's to nothing\n",
    "        temp = re.sub(r\"'s\", \"\", line)\n",
    "        # replacing 've to have\n",
    "        temp = re.sub(r\"'ve\", ' have', temp)\n",
    "        # replacing 're to are\n",
    "        temp = re.sub(r\"'re\", ' are', temp)\n",
    "        # replacing n't to not\n",
    "        temp = re.sub(r\"n't\", ' not', temp)\n",
    "        \n",
    "        # replacing 're to are\n",
    "        temp = re.sub(r\"'d\", ' would', temp)\n",
    "        # replacing n't to not\n",
    "        temp = re.sub(r\"'ll\", ' will', temp)\n",
    "        # replacing 're to are\n",
    "        temp = re.sub(r\"'m\", ' am', temp)\n",
    "        \n",
    "        new_lines.append(temp)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.14186429977417\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with mp.Pool(n_cores) as p:\n",
    "    arc_lines_expanded = p.map(expand_contraction, arc_lines_chunk)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Cleaning Text (e.g. urls and punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(lines):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        # convert all tweets to lower case\n",
    "        temp = line.lower()\n",
    "        # remove www and http URLs\n",
    "        temp = re.sub('((www.\\S+)|(http\\S+))','',temp)\n",
    "        # some words have \"an- tlu-opologist\"\n",
    "        temp = temp.replace(\"- \", \"\")\n",
    "        # some words have \"href\\\\\"\n",
    "        temp = temp.replace(\"\\\\\", \"\")\n",
    "        \n",
    "        temp = re.sub(pattern=r'[{}]'.format(string.punctuation), \n",
    "                repl='', \n",
    "                string=temp\n",
    "               ).strip()\n",
    "        \n",
    "        new_lines.append(temp)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.61854815483093\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with mp.Pool(n_cores) as p:\n",
    "    arc_lines_cleaned = p.map(cleaning_text, arc_lines_expanded)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Tokenizing sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_sentences(lines):\n",
    "    new_lines = []\n",
    "    for line in lines:\n",
    "        temp = line.split()\n",
    "        \n",
    "        if temp != [] and temp != ['']:\n",
    "            new_lines.append(temp)\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.1732075214386\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with mp.Pool(n_cores) as p:\n",
    "    arc_lines_tokenized = p.map(tokenizing_sentences, arc_lines_cleaned)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Finding/Consolidating Additional Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contractions(lines):\n",
    "    new_list = []\n",
    "    punctuations = set(string.punctuation)\n",
    "    \n",
    "    for line in lines:\n",
    "        new_line = []\n",
    "        for word in line:\n",
    "            if any([char in word for char in punctuations]):\n",
    "                new_line.append(word)\n",
    "            \n",
    "        new_list.append(new_line)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.70624136924744\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with mp.Pool(n_cores) as p:\n",
    "    contractions_chunk = p.map(find_contractions, arc_lines_tokenized)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "lists_of_contractions = []\n",
    "for chunk in contractions_chunk:\n",
    "    for index, line in enumerate(chunk[0:50]):\n",
    "        lists_of_contractions += line\n",
    "\n",
    "print(len(lists_of_contractions))\n",
    "lists_of_contractions = set(lists_of_contractions)\n",
    "print(len(lists_of_contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(lines):\n",
    "    new_list = []\n",
    "    for line in lines:\n",
    "        stop_list = set(stopwords.words('english'))\n",
    "        # remove all stop words and not null\n",
    "        words = [token.strip() for token in line if token.strip() not in stop_list and token.strip() != '']\n",
    "        new_list.append(words)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.5580952167511\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "with mp.Pool(n_cores) as p:\n",
    "    arc_lines_stopwords = p.map(stopword_removal, arc_lines_tokenized)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Generate DataFrame (For Data Exploration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_lines_tokenized = list(itertools.chain(*arc_lines_tokenized))\n",
    "arc_lines_stopwords = list(itertools.chain(*arc_lines_stopwords))\n",
    "\n",
    "arc_lines = {\n",
    "    \"tokenized\": arc_lines_tokenized,\n",
    "    \"stopwords_cleaned\": arc_lines_stopwords\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_len'] = df['tokenized'].apply(lambda x: len(x))\n",
    "df['stopwords_len'] = df['stopwords_cleaned'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>stopwords_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14621720.00</td>\n",
       "      <td>14621720.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.22</td>\n",
       "      <td>9.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.52</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.00</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22.00</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>616.00</td>\n",
       "      <td>616.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tokenized_len  stopwords_len\n",
       "count    14621720.00    14621720.00\n",
       "mean           16.22           9.75\n",
       "std            12.52           7.74\n",
       "min             1.00           0.00\n",
       "25%             7.00           5.00\n",
       "50%            14.00           8.00\n",
       "75%            22.00          13.00\n",
       "max           616.00         616.00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.describe(),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Remove sentences with less than 5 words (stop words removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Filtering: 14621720\n",
      "After Filtering: 10052037\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Filtering:\", df.shape[0])\n",
    "filtered_df = df[df['stopwords_len'] > 5]\n",
    "print(\"After Filtering:\", filtered_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YlUhfCnyk2Yl"
   },
   "source": [
    "## 5.2 Generating a subset of the dataset (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_len</th>\n",
       "      <th>stopwords_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>42.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>616.0</td>\n",
       "      <td>616.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tokenized_len  stopwords_len\n",
       "0.90           35.0           20.0\n",
       "0.95           42.0           25.0\n",
       "1.00          616.0          616.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.quantile([0.9, 0.95, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530201"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_percentile_df = filtered_df[filtered_df['stopwords_len'] >= 25]\n",
    "top_5_percentile_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Saving the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_percentile_df.to_csv(os.path.join(interim_folder, 'corpus_subset.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "is425-text-mining-arc2-data-cleaning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
