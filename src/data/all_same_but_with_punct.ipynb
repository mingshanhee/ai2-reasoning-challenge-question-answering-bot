{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"all_same_but_with_punct.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wPsPdXFrKesl"},"source":["# 1. Mounting Google Drive\n","Follow this guide (using last method, \"Bonus Method\"):\n","https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92 \n","\n","Step 1: Add \"IS425 Text Mining Dataset\" folder from our shared folder into \"My Drive\" (right-click TM folder > \"add to My Drive\")"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VIZU9DpEKmIn","colab":{}},"source":["from google.colab import drive\n","drive.mount('drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p8FEJwmbnlzP","colab":{}},"source":["%cd \"drive/My Drive\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gkR2KdCpmh9_"},"source":["# 2. Specifying Dependencies"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9R2ksQqndE6o","colab":{}},"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import multiprocessing as mp\n","\n","from statistics import mean\n","from scipy import stats\n","\n","import nltk\n","import os\n","import time\n","\n","import math\n","import re\n","import string\n","\n","import itertools\n","\n","\n","# The following statement imports a class called PlaintextCorpusReader.\n","from nltk.corpus import PlaintextCorpusReader\n","\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"htyixzsLEEdj","outputId":"20fb394e-41ee-4bd4-96e2-af5448e0f483","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YjqOr4kRj0qq"},"source":["# 3. Loading Dataset"]},{"cell_type":"markdown","metadata":{"id":"UKeYRfn79Mfe","colab_type":"text"},"source":["## 3.1 RAW Files"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7C-8X-Bg_x2a","colab":{}},"source":["# #Run if need access to corpus. Separated it because it takes a long time to run\n","raw_folder = \"/content/drive/My Drive/IS 425 Text Mining Dataset/ARC/raw/\"\n","interim_folder = \"/content/drive/My Drive/IS 425 Text Mining Dataset/ARC/interim/\"\n","arc_corpus_filename = \"ARC_Corpus.txt\"\n","\n","# raw_folder = \"../../data/raw/\"\n","# interim_folder = \"../../data/interim\"\n","# arc_corpus_filename = \"ARC_Corpus.txt\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SwTKpgFeQa2O","colab":{}},"source":["f = open(os.path.join(raw_folder, arc_corpus_filename), \"r\")\n","\n","arc_lines = f.read().splitlines()\n","\n","f.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RskmpjaC9MgX","colab_type":"text"},"source":["## 3.2 Splitting the data into chunks"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zc176cjQQ3fq","colab":{}},"source":["n_cores = 60\n","chunk_size = math.ceil(len(arc_lines) / n_cores)\n","chunk_size = int(chunk_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ys1H65AA9Mg5","colab_type":"code","colab":{},"outputId":"9fa12cfd-d1df-4a9c-e6bc-3dc75938db46"},"source":["def chunks(lst, n):\n","    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n","    for i in range(0, len(lst), n):\n","        yield lst[i:i + n]\n","\n","arc_lines_chunk =list(chunks(arc_lines, chunk_size))\n","print(len(arc_lines_chunk))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["60\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oHsJx7kP9MhX","colab_type":"code","colab":{},"outputId":"a7470ffb-aaba-40a9-a75a-d3ab9886e0dc"},"source":["arc_lines[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Large international companies are involved in bauxite, iron ore, diamond, and gold mining operations.'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"ShtDbxii9Mhy","colab_type":"code","colab":{},"outputId":"a6259efa-e9ac-41e6-ba46-f7d8cdfd000b"},"source":["arc_lines[1]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Paleoceanography, 8(2): 193-208.'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eAkF08fEMY1q"},"source":["# 4. Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"Ua2PYTFx9MiT","colab_type":"text"},"source":["## 4.1 Expand Contractions"]},{"cell_type":"code","metadata":{"id":"NObyzYgz9Mib","colab_type":"code","colab":{}},"source":["def expand_contraction(lines):\n","    new_lines = []\n","    for line in lines:\n","            \n","        # replacing 's to nothing\n","        temp = re.sub(r\"'s\", \"\", line)\n","        # replacing 've to have\n","        temp = re.sub(r\"'ve\", ' have', temp)\n","        # replacing 're to are\n","        temp = re.sub(r\"'re\", ' are', temp)\n","        # replacing n't to not\n","        temp = re.sub(r\"n't\", ' not', temp)\n","        \n","        # replacing 're to are\n","        temp = re.sub(r\"'d\", ' would', temp)\n","        # replacing n't to not\n","        temp = re.sub(r\"'ll\", ' will', temp)\n","        # replacing 're to are\n","        temp = re.sub(r\"'m\", ' am', temp)\n","        \n","        new_lines.append(temp)\n","    return new_lines"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHCL7LV19Miw","colab_type":"code","colab":{},"outputId":"65f90a49-c501-495c-cfb3-18ac3a5b5b69"},"source":["start = time.time()\n","\n","with mp.Pool(n_cores) as p:\n","    arc_lines_expanded = p.map(expand_contraction, arc_lines_chunk)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["31.14186429977417\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_4hSeIi59MjK","colab_type":"text"},"source":["## 4.2 Cleaning Text (e.g. urls and punctuations)"]},{"cell_type":"code","metadata":{"id":"DOl9N09Y9MjQ","colab_type":"code","colab":{}},"source":["def cleaning_text(lines):\n","    new_lines = []\n","    for line in lines:\n","        # convert all tweets to lower case\n","        temp = line.lower()\n","        # remove www and http URLs\n","        temp = re.sub('((www.\\S+)|(http\\S+))','',temp)\n","        # some words have \"an- tlu-opologist\"\n","        temp = temp.replace(\"- \", \"\")\n","        # some words have \"href\\\\\"\n","        temp = temp.replace(\"\\\\\", \"\")\n","        \n","        # temp = re.sub(pattern=r'[{}]'.format(string.punctuation), \n","        #         repl='', \n","        #         string=temp\n","        #        ).strip()\n","        \n","        new_lines.append(temp)\n","    return new_lines"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KM4f6pte9Mjj","colab_type":"code","colab":{},"outputId":"c6f5c15b-62a9-44b1-835a-43e933faacd0"},"source":["start = time.time()\n","\n","with mp.Pool(n_cores) as p:\n","    arc_lines_cleaned = p.map(cleaning_text, arc_lines_expanded)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["32.61854815483093\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0cvTzgDk9Mj5","colab_type":"text"},"source":["## 4.3 Tokenizing sentences into words"]},{"cell_type":"code","metadata":{"id":"oPY6CVuE9Mj_","colab_type":"code","colab":{}},"source":["def tokenizing_sentences(lines):\n","    new_lines = []\n","    for line in lines:\n","        temp = line.split()\n","        \n","        if temp != [] and temp != ['']:\n","            new_lines.append(temp)\n","    return new_lines"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOHP2XrI9MkR","colab_type":"code","colab":{},"outputId":"a0c76dd0-bdef-4706-de26-43f0a4e0728e"},"source":["start = time.time()\n","\n","with mp.Pool(n_cores) as p:\n","    arc_lines_tokenized = p.map(tokenizing_sentences, arc_lines_cleaned)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["213.1732075214386\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WC_GejzC9Mkj","colab_type":"text"},"source":["## 4.4 Finding/Consolidating Additional Contractions"]},{"cell_type":"code","metadata":{"id":"sDgJKGnT9Mko","colab_type":"code","colab":{}},"source":["def find_contractions(lines):\n","    new_list = []\n","    punctuations = set(string.punctuation)\n","    \n","    for line in lines:\n","        new_line = []\n","        for word in line:\n","            if any([char in word for char in punctuations]):\n","                new_line.append(word)\n","            \n","        new_list.append(new_line)\n","    return new_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUU7DFKM9Mk5","colab_type":"code","colab":{},"outputId":"b1200278-e1ec-413b-8023-bd4c6ec12ff6"},"source":["start = time.time()\n","\n","with mp.Pool(n_cores) as p:\n","    contractions_chunk = p.map(find_contractions, arc_lines_tokenized)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["200.70624136924744\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xejcUc8t9MlQ","colab_type":"code","colab":{},"outputId":"c5121f72-d654-410d-92ce-cd4f3d71cc2e"},"source":["lists_of_contractions = []\n","for chunk in contractions_chunk:\n","    for index, line in enumerate(chunk[0:50]):\n","        lists_of_contractions += line\n","\n","print(len(lists_of_contractions))\n","lists_of_contractions = set(lists_of_contractions)\n","print(len(lists_of_contractions))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eJxDAohO9Mlc","colab_type":"text"},"source":["## 4.5 Stopword Removal"]},{"cell_type":"code","metadata":{"id":"Xad2sKb99Mlk","colab_type":"code","colab":{}},"source":["def stopword_removal(lines):\n","    new_list = []\n","    for line in lines:\n","        stop_list = set(stopwords.words('english'))\n","        # remove all stop words and not null\n","        words = [token.strip() for token in line if token.strip() not in stop_list and token.strip() != '']\n","        new_list.append(words)\n","    return new_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MGV7N4n9Mlz","colab_type":"code","colab":{},"outputId":"8b96ad18-9661-453a-dca5-9ec5fb512ae3"},"source":["start = time.time()\n","\n","with mp.Pool(n_cores) as p:\n","    arc_lines_stopwords = p.map(stopword_removal, arc_lines_tokenized)\n","\n","end = time.time()\n","print(end - start)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["231.5580952167511\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Frh3mGyn9MmD","colab_type":"text"},"source":["# 5 Generate DataFrame (For Data Exploration)"]},{"cell_type":"code","metadata":{"id":"VDsAxolD9MmH","colab_type":"code","colab":{}},"source":["arc_lines_tokenized = list(itertools.chain(*arc_lines_tokenized))\n","arc_lines_stopwords = list(itertools.chain(*arc_lines_stopwords))\n","\n","arc_lines = {\n","    \"tokenized\": arc_lines_tokenized,\n","    \"stopwords_cleaned\": arc_lines_stopwords\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_tkO3Ll9MmV","colab_type":"code","colab":{}},"source":["df = pd.DataFrame(arc_lines)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2CcrVRu9Mmk","colab_type":"code","colab":{}},"source":["df['tokenized_len'] = df['tokenized'].apply(lambda x: len(x))\n","df['stopwords_len'] = df['stopwords_cleaned'].apply(lambda x: len(x))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gnE6LCAk9Mmw","colab_type":"code","colab":{},"outputId":"9a566625-c7f0-462d-d458-469cb3ae9cb2"},"source":["round(df.describe(),2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokenized_len</th>\n","      <th>stopwords_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>14621720.00</td>\n","      <td>14621720.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>16.22</td>\n","      <td>9.75</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>12.52</td>\n","      <td>7.74</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>7.00</td>\n","      <td>5.00</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>14.00</td>\n","      <td>8.00</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>22.00</td>\n","      <td>13.00</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>616.00</td>\n","      <td>616.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       tokenized_len  stopwords_len\n","count    14621720.00    14621720.00\n","mean           16.22           9.75\n","std            12.52           7.74\n","min             1.00           0.00\n","25%             7.00           5.00\n","50%            14.00           8.00\n","75%            22.00          13.00\n","max           616.00         616.00"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"vuGFydgO9MnI","colab_type":"text"},"source":["## 5.1 Remove sentences with less than 5 words (stop words removed)"]},{"cell_type":"code","metadata":{"id":"X4EFbmU59MnO","colab_type":"code","colab":{},"outputId":"8e11ea77-00c6-4c05-a350-53e86c2d3c9b"},"source":["print(\"Before Filtering:\", df.shape[0])\n","filtered_df = df[df['stopwords_len'] > 5]\n","print(\"After Filtering:\", filtered_df.shape[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Before Filtering: 14621720\n","After Filtering: 10052037\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YlUhfCnyk2Yl","colab":{}},"source":["## 5.2 Generating a subset of the dataset (5%)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7KWiqWw9Mns","colab_type":"code","colab":{},"outputId":"d26e200e-6526-49fb-8613-a99f1a5a23c0"},"source":["filtered_df.quantile([0.9, 0.95, 1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tokenized_len</th>\n","      <th>stopwords_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0.90</th>\n","      <td>35.0</td>\n","      <td>20.0</td>\n","    </tr>\n","    <tr>\n","      <th>0.95</th>\n","      <td>42.0</td>\n","      <td>25.0</td>\n","    </tr>\n","    <tr>\n","      <th>1.00</th>\n","      <td>616.0</td>\n","      <td>616.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      tokenized_len  stopwords_len\n","0.90           35.0           20.0\n","0.95           42.0           25.0\n","1.00          616.0          616.0"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"WsVYcSnq9Mn6","colab_type":"code","colab":{},"outputId":"20ed3372-efcf-4b28-e9ae-838683800642"},"source":["top_5_percentile_df = filtered_df[filtered_df['stopwords_len'] >= 25]\n","top_5_percentile_df.shape[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["530201"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"R_FVs37Q9MoH","colab_type":"text"},"source":["## 5.3 Saving the subset"]},{"cell_type":"code","metadata":{"id":"Zc6W0BDs9MoL","colab_type":"code","colab":{}},"source":["top_5_percentile_df.to_csv(os.path.join(interim_folder, 'corpus_subset_5_percent_w_punct.csv'), index=False)"],"execution_count":0,"outputs":[]}]}