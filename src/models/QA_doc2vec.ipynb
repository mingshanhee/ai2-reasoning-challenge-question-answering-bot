{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_doc2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kciH3NztJWxX",
        "colab_type": "code",
        "outputId": "8c94c014-cbd2-4739-e827-69a16b6eeec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QneuuedxMwkE",
        "colab_type": "code",
        "outputId": "fbb0c3ab-8a28-44e9-d807-a41779d54019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjOwkXXZMjy0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3527e2e-6113-4113-a252-1ceae6e38ddf"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install -U git+https://github.com/oborchers/Fast_Sentence_Embeddings"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.31)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.18.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.31)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.1)\n",
            "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (46.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.2.8)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.10.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.51.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.8)\n",
            "Collecting git+https://github.com/oborchers/Fast_Sentence_Embeddings\n",
            "  Cloning https://github.com/oborchers/Fast_Sentence_Embeddings to /tmp/pip-req-build-kulawxac\n",
            "  Running command git clone -q https://github.com/oborchers/Fast_Sentence_Embeddings /tmp/pip-req-build-kulawxac\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from fse==0.1.16) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from fse==0.1.16) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: smart_open>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from fse==0.1.16) (1.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from fse==0.1.16) (0.22.2.post1)\n",
            "Collecting gensim>=3.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
            "\u001b[?25hCollecting wordfreq>=2.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7f/029a2d22362e785a258cd8bd5725f453817decfb31ac5d6dff0c472303d3/wordfreq-2.2.2.tar.gz (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 94kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: psutil in /usr/local/lib/python3.6/dist-packages (from fse==0.1.16) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart_open>=1.5.0->fse==0.1.16) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart_open>=1.5.0->fse==0.1.16) (1.12.31)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart_open>=1.5.0->fse==0.1.16) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->fse==0.1.16) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.8.0->fse==0.1.16) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: msgpack in /usr/local/lib/python3.6/dist-packages (from wordfreq>=2.2.1->fse==0.1.16) (1.0.0)\n",
            "Collecting langcodes>=1.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/9a/e05169c2c00b11b21fb0af039644fa07210470a125aa508a460786c2e63f/langcodes-1.4.1.tar.gz (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 44.9MB/s \n",
            "\u001b[?25hCollecting regex<=2018.02.21,>=2017.07.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/51/c39562cfed3272592c60cfd229e5464d715b78537e332eac2b695422dc49/regex-2018.02.21.tar.gz (620kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 61.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (1.0.3)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open>=1.5.0->fse==0.1.16) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.31 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open>=1.5.0->fse==0.1.16) (1.15.31)\n",
            "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open>=1.5.0->fse==0.1.16) (0.9.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open>=1.5.0->fse==0.1.16) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open>=1.5.0->fse==0.1.16) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open>=1.5.0->fse==0.1.16) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open>=1.5.0->fse==0.1.16) (1.24.3)\n",
            "Collecting marisa-trie\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/95/d23071d0992dabcb61c948fb118a90683193befc88c23e745b050a29e7db/marisa-trie-0.7.5.tar.gz (270kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 71.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (1.16.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->smart_open>=1.5.0->fse==0.1.16) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.31->boto3->smart_open>=1.5.0->fse==0.1.16) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (1.51.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.2.0->google-cloud-storage->smart_open>=1.5.0->fse==0.1.16) (0.4.8)\n",
            "Building wheels for collected packages: fse, wordfreq, langcodes, regex, marisa-trie\n",
            "  Building wheel for fse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fse: filename=fse-0.1.16-cp36-cp36m-linux_x86_64.whl size=607163 sha256=d3918f64704308780efa9bc454a5ae51efc7b240bccf80a32d8ea5f275a645b0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d7kk83bz/wheels/03/f8/47/e3513fb3e2032a52d8648ab9f06827e2811421aa3b7cbc7447\n",
            "  Building wheel for wordfreq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordfreq: filename=wordfreq-2.2.2-cp36-none-any.whl size=32816665 sha256=333259818e33dc036b899689452f6228aba4b9cd24e7409e0fa59e6d564a86c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/2e/fc/e447859743f61cdf41873a5bcc11300c05fbd27631aea984e1\n",
            "  Building wheel for langcodes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langcodes: filename=langcodes-1.4.1-cp36-none-any.whl size=4100892 sha256=a20af2612bf9827a287d427e3902c4f3e344cbcd56ef54daa2133461a56eb725\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/20/3d/dc2010b4f7c0b786a06947530a962972caead0c58898f25a02\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2018.2.21-cp36-cp36m-linux_x86_64.whl size=552306 sha256=8de1ba88fb79f35c1f9b12f52d20fb3e5d2fa3019405ea795f6940f3998d4338\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/c9/cf/230425cdd343d6b98e8da5a5841c3dab1e0c8aaa134e29edb0\n",
            "  Building wheel for marisa-trie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for marisa-trie: filename=marisa_trie-0.7.5-cp36-cp36m-linux_x86_64.whl size=861156 sha256=4351230d0ff2ade94415c45158b2ff7c524c0753faf95bd0a87a52808424cf75\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/24/79/022624fc914f0e559fe8a1141aaff1f9df810905a13fc75d57\n",
            "Successfully built fse wordfreq langcodes regex marisa-trie\n",
            "Installing collected packages: gensim, marisa-trie, langcodes, regex, wordfreq, fse\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed fse-0.1.16 gensim-3.8.1 langcodes-1.4.1 marisa-trie-0.7.5 regex-2018.2.21 wordfreq-2.2.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "regex"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZytYfFkJYXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag,word_tokenize,ne_chunk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tree import Tree\n",
        "from nltk import pos_tag,ne_chunk\n",
        "\n",
        "from fse import IndexedList\n",
        "import gensim.downloader as api\n",
        "from fse.models.average import FAST_VERSION, MAX_WORDS_IN_BATCH\n",
        "from fse.models import uSIF\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora, similarities, models\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_list = stopwords.words('english')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Oq7rzhKnKa",
        "colab_type": "text"
      },
      "source": [
        "##Doc2Vec on 5% corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlmGo4AUIM9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/TM/Project/data/ARC/interim/corpus_subset_5_percent.csv\"\n",
        "corpus = pd.read_csv(path)\n",
        "docs = [ast.literal_eval(sent) for sent in corpus['stopwords_cleaned']]\n",
        "stem = [[stemmer.stem(w) for w in doc] for doc in docs] #stemmng\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEBfs9XGzCMy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59825b3a-7df6-4d3d-9f17-169669ed009a"
      },
      "source": [
        "# pre-trained\n",
        "glove = api.load(\"glove-wiki-gigaword-100\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAxeGlbyNMcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "be09e5bb-4a09-420a-a03f-cfe3fe918fc9"
      },
      "source": [
        "print(MAX_WORDS_IN_BATCH)\n",
        "print(FAST_VERSION) # 1 -> The fast version works"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG2i28wEIM3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09071899-c51f-4689-ff58-ea14bcc2998c"
      },
      "source": [
        "s = IndexedList(docs)\n",
        "model = uSIF(glove, workers=2, lang_freq=\"en\")\n",
        "model.train(s)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(517983, 15055906)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoL3oJAGNP-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"sent2vec.pkl\", \"wb\") as file:\n",
        "  pickle.dump(model, file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHHYxNZ6NQEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open(\"sent2vec.pkl\", \"rb\") as r:\n",
        "#   model = pickle.load(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLluo84GIKaw",
        "colab_type": "text"
      },
      "source": [
        "##DateExtractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PW1B8KK04R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for tagging temporal expressions in text\n",
        "# For details of the TIMEX format, see http://timex2.mitre.org/\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Requires eGenix.com mx Base Distribution\n",
        "# http://www.egenix.com/products/python/mxBase/\n",
        "\n",
        "# Predefined strings.\n",
        "numbers = \"(^a(?=\\s)|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand)\"\n",
        "day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n",
        "week_day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n",
        "month = \"(january|february|march|april|may|june|july|august|september|october|november|december)\"\n",
        "dmy = \"(year|day|week|month)\"\n",
        "rel_day = \"(today|yesterday|tomorrow|tonight|tonite)\"\n",
        "exp1 = \"(before|after|earlier|later|ago)\"\n",
        "exp2 = \"(this|next|last)\"\n",
        "iso = \"\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+\"\n",
        "year = \"((?<=\\s)\\d{4}|^\\d{4})\"\n",
        "regxp1 = \"((\\d+|(\" + numbers + \"[-\\s]?)+) \" + dmy + \"s? \" + exp1 + \")\"\n",
        "regxp2 = \"(\" + exp2 + \" (\" + dmy + \"|\" + week_day + \"|\" + month + \"))\"\n",
        "\n",
        "date = \"([012]?[0-9]|3[01])\"\n",
        "regxp3 = \"(\" + date + \" \" + month + \" \" + year + \")\"\n",
        "regxp4 = \"(\" + month + \" \" + date + \"[th|st|rd]?[,]? \" + year + \")\"\n",
        "\n",
        "reg1 = re.compile(regxp1, re.IGNORECASE)\n",
        "reg2 = re.compile(regxp2, re.IGNORECASE)\n",
        "reg3 = re.compile(rel_day, re.IGNORECASE)\n",
        "reg4 = re.compile(iso)\n",
        "reg5 = re.compile(year)\n",
        "reg6 = re.compile(regxp3, re.IGNORECASE)\n",
        "reg7 = re.compile(regxp4, re.IGNORECASE)\n",
        "\n",
        "def extractDate(text):\n",
        "\n",
        "    # Initialization\n",
        "    timex_found = []\n",
        "\n",
        "    # re.findall() finds all the substring matches, keep only the full\n",
        "    # matching string. Captures expressions such as 'number of days' ago, etc.\n",
        "    found = reg1.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # Variations of this thursday, next year, etc\n",
        "    found = reg2.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # today, tomorrow, etc\n",
        "    found = reg3.findall(text)\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # ISO\n",
        "    found = reg4.findall(text)\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # Dates\n",
        "    found = reg6.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    found = reg7.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # Year\n",
        "    found = reg5.findall(text)\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "    # Tag only temporal expressions which haven't been tagged.\n",
        "    #for timex in timex_found:\n",
        "    #    text = re.sub(timex + '(?!</TIMEX2>)', '<TIMEX2>' + timex + '</TIMEX2>', text)\n",
        "\n",
        "    return timex_found"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwdRJVo9KIjC",
        "colab_type": "text"
      },
      "source": [
        "##ProcessedQuestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ZigFyvJreV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ScriptName : ProcessedQuestion.py\n",
        "# Description : Takes question as an input and process it to find out question\n",
        "#   and answer type, also prepare question vector and prepare search query for\n",
        "#   Information Retrieval process\n",
        "# Arguments : \n",
        "#       Input :\n",
        "#           question(str) : String of question\n",
        "#           useStemmer(boolean) : Indicate to use stemmer for question tokens\n",
        "#           useSynonyms(boolean) : Indicate to use thesaraus for query expansion\n",
        "#           removeStopwords(boolean) : Indicate to remove stop words from search\n",
        "#                                      query\n",
        "#       Output :\n",
        "#           Instance of ProcessedQuestion with useful following structure\n",
        "#               qVector(list) : doc2vec vector\n",
        "#               question(str) : Raw question\n",
        "#               qType(str) : Type of question\n",
        "#               aType(str) : Expected answer type\n",
        "#                       [\"PERSON\",\"LOCATION\",\"DATE\",\"DEFINITION\",\"YESNO\"]\n",
        "#  \n",
        "\n",
        "class ProcessedQuestion:\n",
        "    def __init__(self, question, useStemmer = False, useSynonyms = False, removeStopwords = False):\n",
        "        self.question = question\n",
        "        self.useStemmer = useStemmer\n",
        "        self.useSynonyms = useSynonyms\n",
        "        self.removeStopwords = removeStopwords\n",
        "        self.stopWords = stopwords.words(\"english\")\n",
        "        self.stem = lambda k : k.lower()\n",
        "        if self.useStemmer:\n",
        "            ps = PorterStemmer()\n",
        "            self.stem = ps.stem\n",
        "        self.qType = self.determineQuestionType(question)\n",
        "        self.searchQuery = self.buildSearchQuery(question)\n",
        "        self.qVector = self.getQueryVector(self.searchQuery)\n",
        "        self.aType = self.determineAnswerType(question)\n",
        "    \n",
        "    # To determine type of question by analyzing POS tag of question from Penn \n",
        "    # Treebank tagset\n",
        "    #\n",
        "    # Input:\n",
        "    #           question(str) : Question string\n",
        "    # Output:\n",
        "    #           qType(str) : Type of question among following\n",
        "    #                   [ WP ->  who\n",
        "    #                     WDT -> what, why, how\n",
        "    #                     WP$ -> whose\n",
        "    #                     WRB -> where ]\n",
        "    def determineQuestionType(self, question):\n",
        "        questionTaggers = ['WP','WDT','WP$','WRB']\n",
        "        qPOS = pos_tag(word_tokenize(question))\n",
        "        qTags = []\n",
        "        for token in qPOS:\n",
        "            if token[1] in questionTaggers:\n",
        "                qTags.append(token[1])\n",
        "        qType = ''\n",
        "        if(len(qTags)>1):\n",
        "            qType = 'complex'\n",
        "        elif(len(qTags) == 1):\n",
        "            qType = qTags[0]\n",
        "        else:\n",
        "            qType = \"None\"\n",
        "        return qType\n",
        "    \n",
        "    # To determine type of expected answer depending of question type\n",
        "    #\n",
        "    # Input:\n",
        "    #           question(str) : Question string\n",
        "    # Output:\n",
        "    #           aType(str) : Type of answer among following\n",
        "    #               [PERSON, LOCATION, DATE, ORGANIZATION, QUANTITY, DEFINITION\n",
        "    #                   FULL]\n",
        "    def determineAnswerType(self, question):\n",
        "        questionTaggers = ['WP','WDT','WP$','WRB']\n",
        "        qPOS = pos_tag(word_tokenize(question))\n",
        "        qTag = None\n",
        "\n",
        "        for token in qPOS:\n",
        "            if token[1] in questionTaggers:\n",
        "                qTag = token[0].lower()\n",
        "                break\n",
        "        \n",
        "        if(qTag == None):\n",
        "            if len(qPOS) > 1:\n",
        "                if qPOS[1][1].lower() in ['is','are','can','should']:\n",
        "                    qTag = \"YESNO\"\n",
        "        #who/where/what/why/when/is/are/can/should\n",
        "        if qTag == \"who\":\n",
        "            return \"PERSON\"\n",
        "        elif qTag == \"where\":\n",
        "            return \"LOCATION\"\n",
        "        elif qTag == \"when\":\n",
        "            return \"DATE\"\n",
        "        elif qTag == \"what\":\n",
        "            # Defination type question\n",
        "            # If question of type whd modal noun? its a defination question\n",
        "            qTok = self.getContinuousChunk(question)\n",
        "            #print(qTok)\n",
        "            if(len(qTok) == 4):\n",
        "                if qTok[1][1] in ['is','are','was','were'] and qTok[2][0] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]:\n",
        "                    self.question = \" \".join([qTok[0][1],qTok[2][1],qTok[1][1]])\n",
        "                    #print(\"Type of question\",\"Definition\",self.question)\n",
        "                    return \"DEFINITION\"\n",
        "\n",
        "            # ELSE USE FIRST HEAD WORD\n",
        "            for token in qPOS:\n",
        "                if token[0].lower() in [\"city\",\"place\",\"country\"]:\n",
        "                    return \"LOCATION\"\n",
        "                elif token[0].lower() in [\"company\",\"industry\",\"organization\"]:\n",
        "                    return \"ORGANIZATION\"\n",
        "                elif token[1] in [\"NN\",\"NNS\"]:\n",
        "                    return \"FULL\"\n",
        "                elif token[1] in [\"NNP\",\"NNPS\"]:\n",
        "                    return \"FULL\"\n",
        "            return \"FULL\"\n",
        "        elif qTag == \"how\":\n",
        "            if len(qPOS)>1:\n",
        "                t2 = qPOS[2]\n",
        "                if t2[0].lower() in [\"few\",\"great\",\"little\",\"many\",\"much\"]:\n",
        "                    return \"QUANTITY\"\n",
        "                elif t2[0].lower() in [\"tall\",\"wide\",\"big\",\"far\"]:\n",
        "                    return \"LINEAR_MEASURE\"\n",
        "            return \"FULL\"\n",
        "        else:\n",
        "            return \"FULL\"\n",
        "    \n",
        "    # To build search query by dropping question word\n",
        "    #\n",
        "    # Input:\n",
        "    #           question(str) : Question string\n",
        "    # Output:\n",
        "    #           searchQuery(list) : List of tokens\n",
        "    def buildSearchQuery(self, question):\n",
        "        qPOS = pos_tag(word_tokenize(question))\n",
        "        searchQuery = []\n",
        "        questionTaggers = ['WP','WDT','WP$','WRB']\n",
        "        for tag in qPOS:\n",
        "            if tag[1] in questionTaggers:\n",
        "                continue\n",
        "            else:\n",
        "                searchQuery.append(tag[0])\n",
        "                if(self.useSynonyms):\n",
        "                    syn = self.getSynonyms(tag[0])\n",
        "                    if(len(syn) > 0):\n",
        "                        searchQuery.extend(syn)\n",
        "        return searchQuery\n",
        "    \n",
        "    # To build query vector\n",
        "    #\n",
        "    # Input:\n",
        "    #       searchQuery(list) : List of tokens from buildSearchQuery method\n",
        "    # Output:\n",
        "    #       qVector(list) : Doc2Vec Vectors\n",
        "    def getQueryVector(self, searchQuery):\n",
        "        if self.removeStopwords:\n",
        "            tokens = [w for w in searchQuery if w not in self.stopWords] \n",
        "        else:\n",
        "            tokens = searchQuery\n",
        "\n",
        "        stem = [self.stem(w) for w in tokens] \n",
        "\n",
        "        query_tmp = (stem,0)\n",
        "        query_sv = model.infer([query_tmp])\n",
        "\n",
        "        return query_sv\n",
        "\n",
        "    \n",
        "    # To get continuous chunk of similar POS tags.\n",
        "    # E.g.  If two NN tags are consequetive, this method will merge and return\n",
        "    #       single NN with combined value.\n",
        "    #       It is helpful in detecting name of single person like John Cena, \n",
        "    #       Steve Jobs\n",
        "    # Input:\n",
        "    #       question(str) : question string\n",
        "    # Output:\n",
        "    #       \n",
        "    def getContinuousChunk(self,question):\n",
        "        chunks = []\n",
        "        answerToken = word_tokenize(question)\n",
        "        nc = pos_tag(answerToken)\n",
        "\n",
        "        prevPos = nc[0][1]\n",
        "        entity = {\"pos\":prevPos,\"chunk\":[]}\n",
        "        for c_node in nc:\n",
        "            (token,pos) = c_node\n",
        "            if pos == prevPos:\n",
        "                prevPos = pos       \n",
        "                entity[\"chunk\"].append(token)\n",
        "            elif prevPos in [\"DT\",\"JJ\"]:\n",
        "                prevPos = pos\n",
        "                entity[\"pos\"] = pos\n",
        "                entity[\"chunk\"].append(token)\n",
        "            else:\n",
        "                if not len(entity[\"chunk\"]) == 0:\n",
        "                    chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "                    entity = {\"pos\":pos,\"chunk\":[token]}\n",
        "                    prevPos = pos\n",
        "        if not len(entity[\"chunk\"]) == 0:\n",
        "            chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "        return chunks\n",
        "    \n",
        "    # To get synonyms of word in order to improve query by using query\n",
        "    # expanision technique\n",
        "    # Input:\n",
        "    #       word(str) : Word token\n",
        "    # Output:\n",
        "    #       synonyms(list) : List of synonyms of given word\n",
        "    def getSynonyms(word):\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for l in syn.lemmas():\n",
        "                w = l.name().lower()\n",
        "                synonyms.extend(w.split(\"_\"))\n",
        "        return list(set(synonyms))\n",
        "    \n",
        "    # String representation of this class\n",
        "    def __repr__(self):\n",
        "        msg = \"Q: \" + self.question + \"\\n\"\n",
        "        msg += \"QType: \" + self.qType + \"\\n\"\n",
        "        msg += \"QVector: \" + str(self.qVector) + \"\\n\"\n",
        "        return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8s8ZpC1N_tO",
        "colab_type": "code",
        "outputId": "f65c0cb0-691a-4bfe-daf5-367c98c6e70d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "ProcessedQuestion('what are you doing?')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Q: what are you doing?\n",
              "QType: WP\n",
              "QVector: [[ 2.27134034e-01  1.62946537e-01  4.24251914e-01 -8.08457017e-01\n",
              "  -4.36500758e-01  2.74209380e-01  5.12070954e-04  6.11309260e-02\n",
              "   4.29269433e-01 -3.19932640e-01  4.11382467e-01 -9.40587670e-02\n",
              "  -6.86478615e-02  2.30929062e-01 -2.15615094e-01 -3.47850859e-01\n",
              "  -3.71870071e-01  5.09612381e-01 -2.81758636e-01  6.20009899e-01\n",
              "  -5.53434342e-03  6.33127570e-01 -2.60619104e-01 -5.10949790e-01\n",
              "   6.10357523e-02  5.93851805e-01 -4.08586919e-01 -6.07120633e-01\n",
              "   1.97147265e-01 -3.22622269e-01 -2.03902181e-02  3.15536708e-01\n",
              "   3.73862296e-01 -1.51439756e-01  9.75634232e-02  2.80474603e-01\n",
              "  -3.41719598e-01 -5.67540526e-04  3.35075438e-01 -4.84775126e-01\n",
              "   1.66079700e-02  2.73438424e-01 -1.49123296e-01 -2.17219025e-01\n",
              "  -8.46209824e-01 -2.92842448e-01 -5.57752103e-02 -4.62295502e-01\n",
              "  -2.56668687e-01 -7.53316224e-01  2.92854235e-02 -3.33959833e-02\n",
              "  -1.83426946e-01  4.63391781e-01 -3.21105480e-01 -6.73570275e-01\n",
              "   4.15688723e-01  4.24040675e-01  2.08086729e-01  1.04958057e-01\n",
              "  -2.47322857e-01  5.97570360e-01 -2.75829494e-01 -3.63245010e-01\n",
              "   2.88634837e-01  1.67922109e-01  7.83869088e-01  2.65943319e-01\n",
              "  -3.23613077e-01  2.54588902e-01  1.19649611e-01 -1.56075478e-01\n",
              "  -2.43783742e-01 -1.96762219e-01  4.02756035e-02  4.55242284e-02\n",
              "   1.66786969e-01  1.23013794e-01 -4.96488810e-03 -5.01483679e-02\n",
              "   3.27249855e-01 -3.45850945e-01 -2.04383880e-01 -2.01453567e-01\n",
              "  -4.84032333e-01  2.99096890e-02 -2.86443710e-01 -1.12444967e-01\n",
              "  -3.06668103e-01 -3.53683800e-01  7.59773478e-02 -1.30493492e-01\n",
              "   4.63879287e-01 -1.32534534e-01 -2.71027148e-01  4.85704280e-03\n",
              "   2.88803369e-01 -2.89572775e-02 -4.59963083e-02  4.16813493e-01]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAqnIS2kKZNj",
        "colab_type": "text"
      },
      "source": [
        "##DocumentRetrievalModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIOgSn00KXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ScriptName : DocumentRetrievalModel.py\n",
        "# Description : Script preprocesses article and paragraph to computer TFIDF.\n",
        "#               Additionally, helps in answer processing \n",
        "# Arguments : \n",
        "#       Input :\n",
        "#           useStemmer(boolean)     : Indicate to use stemmer for word tokens\n",
        "#           removeStopWord(boolean) : Indicate to remove stop words from \n",
        "#                                     paragraph in order to keep relevant words\n",
        "#       Output :\n",
        "#           Instance of DocumentRetrievalModel with following structure\n",
        "#               query(function) : Take instance of processedQuestion and return\n",
        "#                                 answer based on IR and Answer Processing\n",
        "#                                 techniques\n",
        "\n",
        "class DocumentRetrievalModel:\n",
        "    def __init__(self, removeStopWord = False,useStemmer = False):\n",
        "        self.stopwords = stopwords.words('english')\n",
        "        self.removeStopWord = removeStopWord\n",
        "        self.useStemmer = useStemmer\n",
        "        self.vData = None\n",
        "        self.stem = lambda k:k.lower()\n",
        "        if(useStemmer):\n",
        "            ps = PorterStemmer()\n",
        "            self.stem = ps.stem\n",
        "            \n",
        "    # To find answer to the question by first finding relevant paragraph, then\n",
        "    # by finding relevant sentence and then by procssing sentence to get answer\n",
        "    # based on expected answer type\n",
        "    # Input:\n",
        "    #           pQ(ProcessedQuestion) : Instance of ProcessedQuestion\n",
        "    # Output:\n",
        "    #           answer(str) : Response of QA System\n",
        "    def query(self,pQ):\n",
        "        \n",
        "        # Get relevant Paragraph\n",
        "        relevantParagraph = self.getSimilarParagraph(pQ.qVector)\n",
        "\n",
        "        # Get All sentences\n",
        "        sentences = []\n",
        "        relevant_para = []\n",
        "        for tup in relevantParagraph:\n",
        "            if tup != None:\n",
        "                p2 = docs[tup[0]]\n",
        "                p2 = \" \".join(p2)\n",
        "                relevant_para.append(p2)\n",
        "                sentences.extend(sent_tokenize(p2))\n",
        "        \n",
        "        # Get Relevant Sentences\n",
        "        if len(sentences) == 0:\n",
        "            return \"Oops! Unable to find answer\"\n",
        "\n",
        "        # Get most relevant sentence using unigram similarity\n",
        "        relevantSentences = self.getMostRelevantSentences(sentences,pQ,1)\n",
        "\n",
        "        # AnswerType\n",
        "        aType = pQ.aType\n",
        "        \n",
        "        # Default Answer\n",
        "        answer = relevantSentences[0][0]\n",
        "        \n",
        "\n",
        "        ps = PorterStemmer()\n",
        "        # For question type looking for Person\n",
        "        if aType == \"PERSON\":\n",
        "            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if entity[0] == \"PERSON\":\n",
        "                    answer = entity[1]\n",
        "                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                    # If any entity is already in question\n",
        "                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                        continue\n",
        "                    break\n",
        "        elif aType == \"LOCATION\":\n",
        "            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if entity[0] == \"GPE\":\n",
        "                    answer = entity[1]\n",
        "                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                    # If any entity is already in question\n",
        "                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                        continue\n",
        "                    break\n",
        "        elif aType == \"ORGANIZATION\":\n",
        "            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if entity[0] == \"ORGANIZATION\":\n",
        "                    answer = entity[1]\n",
        "                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                    # If any entity is already in question\n",
        "                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                        continue\n",
        "                    break\n",
        "        elif aType == \"DATE\":\n",
        "            allDates = []\n",
        "            for s in relevantSentences:\n",
        "                allDates.extend(extractDate(s[0]))\n",
        "            if len(allDates)>0:\n",
        "                answer = allDates[0]\n",
        "        elif aType in [\"NN\",\"NNP\"]:\n",
        "            candidateAnswers = []\n",
        "            ne = self.getContinuousChunk([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if aType == \"NN\":\n",
        "                    if entity[0] == \"NN\" or entity[0] == \"NNS\":\n",
        "                        answer = entity[1]\n",
        "                        answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                        qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                        # If any entity is already in question\n",
        "                        if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                            continue\n",
        "                        break\n",
        "                elif aType == \"NNP\":\n",
        "                    if entity[0] == \"NNP\" or entity[0] == \"NNPS\":\n",
        "                        answer = entity[1]\n",
        "                        answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                        qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                        # If any entity is already in question\n",
        "                        if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                            continue\n",
        "                        break\n",
        "        elif aType == \"DEFINITION\":\n",
        "            relevantSentences = self.getMostRelevantSentences(sentences,pQ,1)\n",
        "            answer = relevantSentences[0][0]\n",
        "        \n",
        "        # print('relevant doc:', answer)\n",
        "        return answer, relevant_para\n",
        "        \n",
        "    # Get top 3 relevant paragraph based on cosine similarity between question \n",
        "    # vector and paragraph vector\n",
        "    # Input :\n",
        "    #       queryVector(list) : TFIDF Vector\n",
        "    #\n",
        "    # Output:\n",
        "    #       pRanking(list) : List of tuple with top 3 paragraph with its\n",
        "    #                        similarity coefficient\n",
        "    def getSimilarParagraph(self,queryVector):    \n",
        "        q1_sims = model.sv.most_similar(queryVector)\n",
        "        # q1_sorted_sims = sorted(enumerate(q1_sims), key = lambda item: -item[1])\n",
        "\n",
        "        return q1_sims[0:5]\n",
        "    \n",
        "    \n",
        "    # Get most relevant sentences using unigram similarity between question\n",
        "    # sentence and sentence in paragraph containing potential answer\n",
        "    # Input:\n",
        "    #       sentences(list)      : List of sentences in order of occurance as in\n",
        "    #                              paragraph\n",
        "    #       pQ(ProcessedQuestion): Instance of processedQuestion\n",
        "    #       nGram(int)           : Value of nGram (default 3)\n",
        "    # Output:\n",
        "    #       relevantSentences(list) : List of tuple with sentence and their\n",
        "    #                                 similarity coefficient\n",
        "    def getMostRelevantSentences(self, sentences, pQ, nGram=3):\n",
        "        relevantSentences = []\n",
        "        for sent in sentences:\n",
        "            sim = 0\n",
        "            sim = self.sim_ngram_sentence(pQ.question,sent,nGram)\n",
        "            relevantSentences.append((sent,sim))\n",
        "        \n",
        "        return sorted(relevantSentences,key=lambda tup:(tup[1],tup[0]),reverse=True)\n",
        "    \n",
        "    # Compute ngram similarity between a sentence and question\n",
        "    # Input:\n",
        "    #       question(str)   : Question string\n",
        "    #       sentence(str)   : Sentence string\n",
        "    #       nGram(int)      : Value of n in nGram\n",
        "    # Output:\n",
        "    #       sim(float)      : Ngram Similarity Coefficient\n",
        "    def sim_ngram_sentence(self, question, sentence,nGram):\n",
        "        #considering stop words as well\n",
        "        ps = PorterStemmer()\n",
        "        getToken = lambda question:[ ps.stem(w.lower()) for w in word_tokenize(question) ]\n",
        "        getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n",
        "        qToken = getToken(question)\n",
        "        sToken = getToken(sentence)\n",
        "\n",
        "        if(len(qToken) > nGram):\n",
        "            q3gram = set(getNGram(qToken,nGram))\n",
        "            s3gram = set(getNGram(sToken,nGram))\n",
        "            if(len(s3gram) < nGram):\n",
        "                return 0\n",
        "            qLen = len(q3gram)\n",
        "            sLen = len(s3gram)\n",
        "            sim = len(q3gram.intersection(s3gram)) / len(q3gram.union(s3gram))\n",
        "            return sim\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    \n",
        "    # Get Named Entity from the sentence in form of PERSON, GPE, & ORGANIZATION\n",
        "    # Input:\n",
        "    #       answers(list)       : List of potential sentence containing answer\n",
        "    # Output:\n",
        "    #       chunks(list)        : List of tuple with entity and name in ranked \n",
        "    #                             order\n",
        "    def getNamedEntity(self,answers):\n",
        "        chunks = []\n",
        "        for answer in answers:\n",
        "            answerToken = word_tokenize(answer)\n",
        "            nc = ne_chunk(pos_tag(answerToken))\n",
        "            entity = {\"label\":None,\"chunk\":[]}\n",
        "            for c_node in nc:\n",
        "                if(type(c_node) == Tree):\n",
        "                    if(entity[\"label\"] == None):\n",
        "                        entity[\"label\"] = c_node.label()\n",
        "                    entity[\"chunk\"].extend([ token for (token,pos) in c_node.leaves()])\n",
        "                else:\n",
        "                    (token,pos) = c_node\n",
        "                    if pos == \"NNP\":\n",
        "                        entity[\"chunk\"].append(token)\n",
        "                    else:\n",
        "                        if not len(entity[\"chunk\"]) == 0:\n",
        "                            chunks.append((entity[\"label\"],\" \".join(entity[\"chunk\"])))\n",
        "                            entity = {\"label\":None,\"chunk\":[]}\n",
        "            if not len(entity[\"chunk\"]) == 0:\n",
        "                chunks.append((entity[\"label\"],\" \".join(entity[\"chunk\"])))\n",
        "        return chunks\n",
        "    \n",
        "    # To get continuous chunk of similar POS tags.\n",
        "    # E.g.  If two NN tags are consequetive, this method will merge and return\n",
        "    #       single NN with combined value.\n",
        "    #       It is helpful in detecting name of single person like John Cena, \n",
        "    #       Steve Jobs\n",
        "    # Input:\n",
        "    #       answers(list) : list of potential sentence string\n",
        "    # Output:\n",
        "    #       chunks(list)  : list of tuple with entity and name in ranked order\n",
        "    def getContinuousChunk(self,answers):\n",
        "        chunks = []\n",
        "        for answer in answers:\n",
        "            answerToken = word_tokenize(answer)\n",
        "            if(len(answerToken)==0):\n",
        "                continue\n",
        "            nc = pos_tag(answerToken)\n",
        "            \n",
        "            prevPos = nc[0][1]\n",
        "            entity = {\"pos\":prevPos,\"chunk\":[]}\n",
        "            for c_node in nc:\n",
        "                (token,pos) = c_node\n",
        "                if pos == prevPos:\n",
        "                    prevPos = pos       \n",
        "                    entity[\"chunk\"].append(token)\n",
        "                elif prevPos in [\"DT\",\"JJ\"]:\n",
        "                    prevPos = pos\n",
        "                    entity[\"pos\"] = pos\n",
        "                    entity[\"chunk\"].append(token)\n",
        "                else:\n",
        "                    if not len(entity[\"chunk\"]) == 0:\n",
        "                        chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "                        entity = {\"pos\":pos,\"chunk\":[token]}\n",
        "                        prevPos = pos\n",
        "            if not len(entity[\"chunk\"]) == 0:\n",
        "                chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "        return chunks\n",
        "        \n",
        "    # def __repr__(self):\n",
        "    #     msg = \"Total Paras \" + str(self.totalParas) + \"\\n\"\n",
        "    #     msg += \"Total Unique Word \" + str(len(self.idf)) + \"\\n\"\n",
        "    #     return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DU6uMR2UGLu",
        "colab_type": "code",
        "outputId": "1201969d-a938-4a0d-b404-a2c4a169f539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "drm = DocumentRetrievalModel()\n",
        "drm.query(ProcessedQuestion('what are you doing?'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('reason telling want make look like bad ferret owner think come long way since daysbut want anybody thinking keeping ferrets pets consider things making choice',\n",
              " ['got good idea guys ask game creators maybe tell maybe maybe riddle maybe joke maybe talk like weirdos like maybe say would spoiler maybe maybe know way called self couldnt find untaken one also talking like wierdo like apples',\n",
              "  'fever hits instead thinking “okay know pretty well heshe i’ll keep distance” like redblooded earthling would think instead say “this person knows get wants maybe she’ll good hang himher”',\n",
              "  'biden well guess yes vitriol exists today stuff teams glad grandkids know sit say god mean respond guy stage mean happy let put another way',\n",
              "  'reason telling want make look like bad ferret owner think come long way since daysbut want anybody thinking keeping ferrets pets consider things making choice',\n",
              "  'person always makes great fuss get attention person says something bad happened called crying wolf people stop bothering pay attention even things really wrong'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3WckUkDVK2w",
        "colab_type": "text"
      },
      "source": [
        "##MCQ matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e1M_MtaV43S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "# return list of tuple of option index and similarity score\n",
        "\n",
        "def get_option(answer, options):\n",
        "\n",
        "    ans_tokens = word_tokenize(answer.lower())\n",
        "    ans_stop = [w for w in ans_tokens if w not in stop_list] \n",
        "    ans_stem = [stemmer.stem(w) for w in ans_stop] \n",
        "    ans_tmp = (ans_stem, 0)\n",
        "    ans_vec = model.infer([ans_tmp])\n",
        "\n",
        "\n",
        "    opt_tokens = [word_tokenize(opt.lower()) for opt in options]\n",
        "    stop_tokens = [[w for w in opt if w not in stop_list] for opt in opt_tokens]\n",
        "    stem_tokens = [[stemmer.stem(w) for w in opt] for opt in stop_tokens]\n",
        "\n",
        "    opt_vecs = []\n",
        "    for opt in stem_tokens:\n",
        "        opt_tmp = (opt, 0)\n",
        "        a = model.infer([opt_tmp])\n",
        "        opt_vecs.append(a)\n",
        "\n",
        "    answers = []\n",
        "    for i, v in enumerate(opt_vecs):\n",
        "        result = 1 - spatial.distance.cosine(ans_vec, v)\n",
        "        answers.append((i, result))\n",
        "\n",
        "    return sorted(answers, key=lambda tup: (tup[1],tup[0]), reverse=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCssVVMJcI8C",
        "colab_type": "text"
      },
      "source": [
        "##Test 1 query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bhTKYNXbgbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drm = DocumentRetrievalModel(True,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uK9VpkxY-Gp",
        "colab_type": "code",
        "outputId": "b11d29de-6940-492f-b56d-a8140afff0d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# userQuery = 'Which of the following is a typical example of a unicellular organism? (A) earthworm (B) bacteria (C) fungi (D) green algae'\n",
        "userQuery = \"What is the role of decomposers in a food chain? (A) They consume other organisms. (B) They break down dead organic matter. (C) They use the Sun's energy to make food. (D) They convert inorganic matter into organic matter.\"\n",
        "output = re.split(\"\\(\\w+\\)\", userQuery)\n",
        "\n",
        "question = output[0]\n",
        "options = output[1:]\n",
        "\n",
        "    # Proocess Question\n",
        "pq = ProcessedQuestion(question,True,False,True) #qns, stem, syn, stopw removal \n",
        "\n",
        "    # Get Response From Bot\n",
        "answer, relevant_docs = drm.query(pq)\n",
        "\n",
        "answers_sim = get_option(answer, options)\n",
        "# response = answers_sim[0][0] #first option\n",
        "answers_sim"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 0.49201035499572754),\n",
              " (1, 0.19106653332710266),\n",
              " (0, 0.1876186579465866),\n",
              " (3, 0.04212656989693642)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnccgwnNpmZ4",
        "colab_type": "code",
        "outputId": "243a6c75-a985-4623-cca1-d9d293517dab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "relevant_docs"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['food chains task make food chains consisting least organisms leaf caterpillar bird cat food chain draw animals adding key words producer consumer herbivore carnivore omnivore done complete food chain favourite celebrity sports person musician actor etc… paragraph underneath explaining live based food chain',\n",
              " 'living things need food energy help grow movea food chain tells living thing gets food living beings dependent energy requirementit also called food web energy pyramida food chain always starts producer cases plant ends predatorit top food chain',\n",
              " 'food chain consumer producer decomposer sun together food chain art project additionally carnivore food chain furthermore animal food chain kids furthermore cartoon ocean food chain also fish food chain clip art including food chain clip art well fish eating food together food chain clip art photo',\n",
              " 'going discuss food webs food chains together makes sense title topic food web rather food chain display food web user enters food chain search box',\n",
              " 'professor jebb chairman government’s public health responsibility deal food network said concerned ‘excessive sizes’ cinema snacks drinks adding ‘high street food chains caterers well entertainment companies role play make easier people make healthy choices']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTb5_pT-cNTN",
        "colab_type": "text"
      },
      "source": [
        "##Get Accuracy on ARC Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkUodkPqcMpO",
        "colab_type": "code",
        "outputId": "504e1bf2-334d-4c61-84d8-28d6e7246d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "test_path = \"/content/drive/My Drive/TM/Project/data/ARC/raw/ARC-Easy-Test.csv\"\n",
        "qa = pd.read_csv(test_path)\n",
        "qa.head(10)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>schoolGrade</th>\n",
              "      <th>year</th>\n",
              "      <th>question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mercury_417466</td>\n",
              "      <td>417466</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which statement best explains why photosynthes...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mercury_7081673</td>\n",
              "      <td>7081673</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which piece of safety equipment is used to kee...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mercury_7239733</td>\n",
              "      <td>7239733</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>Meiosis is a type of cell division in which ge...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYSEDREGENTS_2015_4_8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>4</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which characteristic describes the texture of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mercury_7037258</td>\n",
              "      <td>7037258</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which best describes the structure of an atom?...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CSZ20679</td>\n",
              "      <td>CSZ20679</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>California Standards Test</td>\n",
              "      <td>8</td>\n",
              "      <td>2009</td>\n",
              "      <td>To express the distance between the Milky Way ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mercury_182158</td>\n",
              "      <td>182158</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>A student has just completed a laboratory acti...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mercury_7216668</td>\n",
              "      <td>7216668</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Students are investigating the effects of diff...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MCAS_2001_5_19</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>Plants use sunlight to make (A) soil. (B) mine...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Mercury_SC_413631</td>\n",
              "      <td>413631</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which of these correctly identifies the way ma...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              questionID originalQuestionID  ...  subject category\n",
              "0         Mercury_417466             417466  ...      NaN     Test\n",
              "1        Mercury_7081673            7081673  ...      NaN     Test\n",
              "2        Mercury_7239733            7239733  ...      NaN     Test\n",
              "3  NYSEDREGENTS_2015_4_8                  8  ...      NaN     Test\n",
              "4        Mercury_7037258            7037258  ...      NaN     Test\n",
              "5               CSZ20679           CSZ20679  ...      NaN     Test\n",
              "6         Mercury_182158             182158  ...      NaN     Test\n",
              "7        Mercury_7216668            7216668  ...      NaN     Test\n",
              "8         MCAS_2001_5_19                 19  ...      NaN     Test\n",
              "9      Mercury_SC_413631             413631  ...      NaN     Test\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge8RBUTHcHg1",
        "colab_type": "code",
        "outputId": "155e1674-ccd6-4100-b2b4-ea280d25b8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "print(qa['totalPossiblePoint'].sum())\n",
        "print(qa['isMultipleChoiceQuestion'].sum())\n",
        "print(len(qa))\n",
        "print(qa['AnswerKey'].value_counts())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2376\n",
            "2376\n",
            "2376\n",
            "C    610\n",
            "A    570\n",
            "B    563\n",
            "D    535\n",
            "4     26\n",
            "1     26\n",
            "3     23\n",
            "2     22\n",
            "E      1\n",
            "Name: AnswerKey, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5j_gD0Adzpw",
        "colab_type": "code",
        "outputId": "b397c68c-ddc4-4271-bd03-e4786e15ae4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "ans_map = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, '1':0, '2':1, '3':2, '4':3}\n",
        "\n",
        "qa = qa.replace({\"AnswerKey\": ans_map})\n",
        "qa['AnswerKey'] = qa['AnswerKey'].astype('int')\n",
        "qa.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>schoolGrade</th>\n",
              "      <th>year</th>\n",
              "      <th>question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mercury_417466</td>\n",
              "      <td>417466</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which statement best explains why photosynthes...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mercury_7081673</td>\n",
              "      <td>7081673</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which piece of safety equipment is used to kee...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mercury_7239733</td>\n",
              "      <td>7239733</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>Meiosis is a type of cell division in which ge...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYSEDREGENTS_2015_4_8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>4</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which characteristic describes the texture of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mercury_7037258</td>\n",
              "      <td>7037258</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which best describes the structure of an atom?...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              questionID originalQuestionID  ...  subject  category\n",
              "0         Mercury_417466             417466  ...      NaN      Test\n",
              "1        Mercury_7081673            7081673  ...      NaN      Test\n",
              "2        Mercury_7239733            7239733  ...      NaN      Test\n",
              "3  NYSEDREGENTS_2015_4_8                  8  ...      NaN      Test\n",
              "4        Mercury_7037258            7037258  ...      NaN      Test\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o6jAcFGe-c4",
        "colab_type": "code",
        "outputId": "23ffc77d-45ba-465a-a12b-4f34d309aeab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(qa['AnswerKey'].value_counts())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2    633\n",
            "0    596\n",
            "1    585\n",
            "3    561\n",
            "4      1\n",
            "Name: AnswerKey, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowiYosCfZ16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate qns and options for input\n",
        "# qns_list = [re.split(\"\\(\\w+\\)\", row)[0] for row in qa['question']]\n",
        "# options_list =  [re.split(\"\\(\\w+\\)\", row)[1:] for row in qa['question']]\n",
        "\n",
        "# qns and options together for input\n",
        "qns_list = [\" \".join(re.split(\"\\(\\w+\\)\", row)) for row in qa['question']]\n",
        "options_list =  [re.split(\"\\(\\w+\\)\", row)[1:] for row in qa['question']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rOPgOBFRlIM",
        "colab_type": "code",
        "outputId": "60d95280-1efa-414b-b058-aca935de333c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "qns_list[0]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Which statement best explains why photosynthesis is the foundation of most food webs?   Sunlight is the source of energy for nearly all ecosystems.   Most ecosystems are found on land instead of in water.   Carbon dioxide is more available than other gases.   The producers in all ecosystems are plants.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXUcksbqX6UP",
        "colab_type": "code",
        "outputId": "e6f789f6-0d51-47d9-8e07-0c32ccfdf48f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "options_list[0]"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Sunlight is the source of energy for nearly all ecosystems. ',\n",
              " ' Most ecosystems are found on land instead of in water. ',\n",
              " ' Carbon dioxide is more available than other gases. ',\n",
              " ' The producers in all ecosystems are plants.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPzV8njigTkW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "59d97ced-e9ed-4fe8-9896-996b8d15343f"
      },
      "source": [
        "predicted_ans = []\n",
        "for i in range(len(qns_list)):\n",
        "    qns = qns_list[i]\n",
        "    mcq = options_list[i]\n",
        "    pq = ProcessedQuestion(qns,True,False,True)\n",
        "    answer, rel_docs = drm.query(pq)\n",
        "    answers_sim = get_option(answer, mcq)\n",
        "    response = answers_sim[0][0] #first option\n",
        "    predicted_ans.append(response)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPaavNsQgTuI",
        "colab_type": "code",
        "outputId": "2eaf8db9-e535-4971-8041-0cc3d9ad53b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# separate qns and options for input\n",
        "from sklearn.metrics import accuracy_score\n",
        "true_ans = qa['AnswerKey'].tolist()\n",
        "accuracy_score(true_ans, predicted_ans)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2828282828282828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzChpYFoRtvk",
        "colab_type": "code",
        "outputId": "8b1ca8db-0326-49d2-bddb-d49bf53c8288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# qns and options together for input\n",
        "from sklearn.metrics import accuracy_score\n",
        "true_ans = qa['AnswerKey'].tolist()\n",
        "accuracy_score(true_ans, predicted_ans)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2946127946127946"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMu0oTQjK8Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"Bot> Please wait, while I am loading my dependencies\")\n",
        "# # from DocumentRetrievalModel import DocumentRetrievalModel as DRM\n",
        "# # from ProcessedQuestion import ProcessedQuestion as PQ\n",
        "# import re\n",
        "# import sys\n",
        "\n",
        "# # if len(sys.argv) == 1:\n",
        "# # \tprint(\"Bot> I need some reference to answer your question\")\n",
        "# # \tprint(\"Bot> Please! Rerun me using following syntax\")\n",
        "# # \tprint(\"\\t\\t$ python3 P2.py <datasetName>\")\n",
        "# # \tprint(\"Bot> You can find dataset name in \\\"dataset\\\" folder\")\n",
        "# # \tprint(\"Bot> Thanks! Bye\")\n",
        "# # \texit()\n",
        "\n",
        "# # datasetName = sys.argv[1]\n",
        "# datasetName = '/content/drive/My Drive/TM self/Factoid-based-Question-Answer-Chatbot/testing3.txt' #change here to corpus txt with \n",
        "# # Loading Dataset\n",
        "# try:\n",
        "# \tdatasetFile = open(datasetName,\"r\")\n",
        "# except FileNotFoundError:\n",
        "# \tprint(\"Bot> Oops! I am unable to locate \\\"\" + datasetName + \"\\\"\")\n",
        "# \texit()\n",
        "\n",
        "# # Retrieving paragraphs : Assumption is that each paragraph in dataset is\n",
        "# # separated by new line character\n",
        "# paragraphs = []\n",
        "# for para in datasetFile.readlines():\n",
        "# \tif(len(para.strip()) > 0):\n",
        "# \t\tparagraphs.append(para.strip())\n",
        "\n",
        "# # Processing Paragraphs\n",
        "# drm = DocumentRetrievalModel(paragraphs,True,True) #docs, stem, stopw removal\n",
        "\n",
        "# print(\"Bot> Hey! I am ready. Ask me factoid based questions only :P\")\n",
        "# print(\"Bot> You can say Bye anytime you want\")\n",
        "\n",
        "# # Greet Pattern\n",
        "# greetPattern = re.compile(\"^\\ *((hi+)|((good\\ )?morning|evening|afternoon)|(he((llo)|y+)))\\ *$\",re.IGNORECASE)\n",
        "\n",
        "# isActive = True\n",
        "# while isActive:\n",
        "#   userQuery = input(\"You> \")\n",
        "#   if not len(userQuery)>0:\n",
        "#     print(\"Bot> You need to ask something\")\n",
        "#   elif greetPattern.findall(userQuery):\n",
        "#     response = \"Hello!\"\n",
        "#   elif (userQuery.strip().lower() == \"bye\"):\n",
        "#     response = \"Bye Bye!\"\n",
        "#     isActive = False\n",
        "#   else:\n",
        "#     # split input into list of qns and options\n",
        "#     output = re.split(\"\\(\\w+\\)\", userQuery)\n",
        "\n",
        "#     question = output[0]\n",
        "#     options = output[1:]\n",
        "\n",
        "# \t\t# Proocess Question\n",
        "#     pq = ProcessedQuestion(question,True,False,True) #qns, stem, syn, stopw removal \n",
        "\n",
        "# \t\t# Get Response From Bot\n",
        "#     answer = drm.query(pq)\n",
        "\n",
        "#     # Get option by comparing with response\n",
        "#     # response = get_option(answer, options)\n",
        "#     answers_sim = get_option(answer, options)\n",
        "#     response = answers_sim[0][0] #first option\n",
        "\n",
        "#   print(\"Bot>\",response)\n",
        "#   # print('Related docs>', answers_sim)\n",
        "#   print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jbFyOMwT9Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyqG-ZbATY9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE0AQC5_TbBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuBRyQu8ZQ8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsqCyojTbIlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls9pmFPPbdME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4kZGJ_7bdF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myV_gYkgTvTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH6wvjpcTvOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM6YYVu9TDk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Which of the following is a typical example of a unicellular organism? (A) earthworm (B) bacteria (C) fungi (D) green algae\n",
        "B\n",
        "\n",
        "What role does the centromere play in cellular reproduction? (A) It is the area where microtubules are formed. (B) It is the area where the nucleus is during cell division. (C) It is the area of alignment for the chromosomes. (D) It is the area of attachment for chromatids.\n",
        "D\n",
        "\n",
        "What causes a blue block to appear blue in the sunlight? (A) The block absorbs all blue light. (B) The block bends (refracts) all blue light. (C) Only blue light is reflected by the block. (D) Only blue light passes through the block.\n",
        "C\n",
        "\n",
        "To safely conduct an experiment using chemicals, what should students always do? (A) Work in large groups. (B) Wear safety goggles. (C) Wear short sleeves. (D) Keep a window open.\n",
        "B\n",
        "\n",
        "Which are two parts of the carbon cycle? (A) freezing and thawing (B) growth and reproduction (C) evaporation and precipitation (D) photosynthesis and respiration\n",
        "D\n",
        "\n",
        "The digestive system breaks food into simple substances that the body can use. What system carries these simple substances from the digestive system to other parts of the body? (A) circulatory (B) nervous (C) respiratory (D) skeletal\n",
        "A\n",
        "\n",
        "What tool would be used to examine a fingerprint? (A) a graduated cylinder (B) a hand lens (C) a pair of goggles (D) a thermometer\n",
        "B\n",
        "\n",
        "Which sense is used to tell if there is sugar in a glass of tea? (A) Touch (B) Hearing (C) Smell (D) Taste\n",
        "D\n",
        "\n",
        "All organisms contain DNA and RNA. What are the subunits of DNA and RNA? (A) simple sugars (B) amino acids (C) carbohydrates (D) nucleotides\n",
        "D\n",
        "\n",
        "What happens during photosynthesis? (A) Insects pollinate plants. (B) Plants change soil into food energy. (C) Animals get carbon dioxide from plants. (D) Plants change light energy into food energy.\n",
        "D\n",
        "\n",
        "What is the role of decomposers in a food chain? (A) They consume other organisms. (B) They break down dead organic matter. (C) They use the Sun's energy to make food. (D) They convert inorganic matter into organic matter.\n",
        "B\n",
        "\n",
        "Which part of the electromagnetic spectrum can humans sense without using equipment or technology? (A) radio waves (B) visible light (C) microwaves (D) X-rays\n",
        "B\n",
        "\n",
        "What does a mirror do to light that causes objects to appear backwards? (A) refracts (B) reflects (C) absorbs (D) blocks\n",
        "B"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}