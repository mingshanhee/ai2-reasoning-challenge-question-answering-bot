{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA TFIDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kciH3NztJWxX",
        "colab_type": "code",
        "outputId": "804ed831-9c04-4145-add4-b4983acb6237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QneuuedxMwkE",
        "colab_type": "code",
        "outputId": "a0a2971b-422f-442b-d18f-bfe80beffb5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZytYfFkJYXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag,word_tokenize,ne_chunk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tree import Tree\n",
        "from nltk import pos_tag,ne_chunk\n",
        "\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora, similarities, models\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stop_list = stopwords.words('english')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Oq7rzhKnKa",
        "colab_type": "text"
      },
      "source": [
        "##TFIDF on 5% corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlmGo4AUIM9c",
        "colab_type": "code",
        "outputId": "a8a4041f-e559-4206-dda7-0b3fdfdccdd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "path = \"/content/drive/My Drive/TM/Project/data/ARC/interim/corpus_subset_5_percent.csv\"\n",
        "corpus = pd.read_csv(path)\n",
        "docs = [ast.literal_eval(sent) for sent in corpus['stopwords_cleaned']]\n",
        "stem = [[stemmer.stem(w) for w in doc] for doc in docs] #stemmng\n",
        "dictionary = corpora.Dictionary(stem)\n",
        "print(dictionary)\n",
        "\n",
        "# Converting all documents to a list of sparse vectors\n",
        "vecs = [dictionary.doc2bow(doc) for doc in stem]\n",
        "\n",
        "#Convert the vecs to tfidf vectors using TfidfModel function\n",
        "tfidf = models.TfidfModel(vecs)\n",
        "vecs_with_tfidf = [tfidf[vec] for vec in vecs]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(948716 unique tokens: ['1427', 'adult', 'aquat', 'chew', 'damselfli']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEBfs9XGzCMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_dict = len(dictionary) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG2i28wEIM3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs_index = similarities.SparseMatrixSimilarity(vecs_with_tfidf, n_dict)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLluo84GIKaw",
        "colab_type": "text"
      },
      "source": [
        "##DateExtractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PW1B8KK04R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for tagging temporal expressions in text\n",
        "# For details of the TIMEX format, see http://timex2.mitre.org/\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Requires eGenix.com mx Base Distribution\n",
        "# http://www.egenix.com/products/python/mxBase/\n",
        "\n",
        "# Predefined strings.\n",
        "numbers = \"(^a(?=\\s)|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety|hundred|thousand)\"\n",
        "day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n",
        "week_day = \"(monday|tuesday|wednesday|thursday|friday|saturday|sunday)\"\n",
        "month = \"(january|february|march|april|may|june|july|august|september|october|november|december)\"\n",
        "dmy = \"(year|day|week|month)\"\n",
        "rel_day = \"(today|yesterday|tomorrow|tonight|tonite)\"\n",
        "exp1 = \"(before|after|earlier|later|ago)\"\n",
        "exp2 = \"(this|next|last)\"\n",
        "iso = \"\\d+[/-]\\d+[/-]\\d+ \\d+:\\d+:\\d+\\.\\d+\"\n",
        "year = \"((?<=\\s)\\d{4}|^\\d{4})\"\n",
        "regxp1 = \"((\\d+|(\" + numbers + \"[-\\s]?)+) \" + dmy + \"s? \" + exp1 + \")\"\n",
        "regxp2 = \"(\" + exp2 + \" (\" + dmy + \"|\" + week_day + \"|\" + month + \"))\"\n",
        "\n",
        "date = \"([012]?[0-9]|3[01])\"\n",
        "regxp3 = \"(\" + date + \" \" + month + \" \" + year + \")\"\n",
        "regxp4 = \"(\" + month + \" \" + date + \"[th|st|rd]?[,]? \" + year + \")\"\n",
        "\n",
        "reg1 = re.compile(regxp1, re.IGNORECASE)\n",
        "reg2 = re.compile(regxp2, re.IGNORECASE)\n",
        "reg3 = re.compile(rel_day, re.IGNORECASE)\n",
        "reg4 = re.compile(iso)\n",
        "reg5 = re.compile(year)\n",
        "reg6 = re.compile(regxp3, re.IGNORECASE)\n",
        "reg7 = re.compile(regxp4, re.IGNORECASE)\n",
        "\n",
        "def extractDate(text):\n",
        "\n",
        "    # Initialization\n",
        "    timex_found = []\n",
        "\n",
        "    # re.findall() finds all the substring matches, keep only the full\n",
        "    # matching string. Captures expressions such as 'number of days' ago, etc.\n",
        "    found = reg1.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # Variations of this thursday, next year, etc\n",
        "    found = reg2.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # today, tomorrow, etc\n",
        "    found = reg3.findall(text)\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # ISO\n",
        "    found = reg4.findall(text)\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # Dates\n",
        "    found = reg6.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    found = reg7.findall(text)\n",
        "    found = [a[0] for a in found if len(a) > 1]\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "\n",
        "    # Year\n",
        "    found = reg5.findall(text)\n",
        "    for timex in found:\n",
        "        timex_found.append(timex)\n",
        "    # Tag only temporal expressions which haven't been tagged.\n",
        "    #for timex in timex_found:\n",
        "    #    text = re.sub(timex + '(?!</TIMEX2>)', '<TIMEX2>' + timex + '</TIMEX2>', text)\n",
        "\n",
        "    return timex_found"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwdRJVo9KIjC",
        "colab_type": "text"
      },
      "source": [
        "##ProcessedQuestion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ZigFyvJreV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ScriptName : ProcessedQuestion.py\n",
        "# Description : Takes question as an input and process it to find out question\n",
        "#   and answer type, also prepare question vector and prepare search query for\n",
        "#   Information Retrieval process\n",
        "# Arguments : \n",
        "#       Input :\n",
        "#           question(str) : String of question\n",
        "#           useStemmer(boolean) : Indicate to use stemmer for question tokens\n",
        "#           useSynonyms(boolean) : Indicate to use thesaraus for query expansion\n",
        "#           removeStopwords(boolean) : Indicate to remove stop words from search\n",
        "#                                      query\n",
        "#       Output :\n",
        "#           Instance of ProcessedQuestion with useful following structure\n",
        "#               qVector(list) : TFIDF vector\n",
        "#               question(str) : Raw question\n",
        "#               qType(str) : Type of question\n",
        "#               aType(str) : Expected answer type\n",
        "#                       [\"PERSON\",\"LOCATION\",\"DATE\",\"DEFINITION\",\"YESNO\"]\n",
        "#  \n",
        "\n",
        "class ProcessedQuestion:\n",
        "    def __init__(self, question, useStemmer = False, useSynonyms = False, removeStopwords = False):\n",
        "        self.question = question\n",
        "        self.useStemmer = useStemmer\n",
        "        self.useSynonyms = useSynonyms\n",
        "        self.removeStopwords = removeStopwords\n",
        "        self.stopWords = stopwords.words(\"english\")\n",
        "        self.stem = lambda k : k.lower()\n",
        "        if self.useStemmer:\n",
        "            ps = PorterStemmer()\n",
        "            self.stem = ps.stem\n",
        "        self.qType = self.determineQuestionType(question)\n",
        "        self.searchQuery = self.buildSearchQuery(question)\n",
        "        self.qVector = self.getQueryVector(self.searchQuery)\n",
        "        self.aType = self.determineAnswerType(question)\n",
        "    \n",
        "    # To determine type of question by analyzing POS tag of question from Penn \n",
        "    # Treebank tagset\n",
        "    #\n",
        "    # Input:\n",
        "    #           question(str) : Question string\n",
        "    # Output:\n",
        "    #           qType(str) : Type of question among following\n",
        "    #                   [ WP ->  who\n",
        "    #                     WDT -> what, why, how\n",
        "    #                     WP$ -> whose\n",
        "    #                     WRB -> where ]\n",
        "    def determineQuestionType(self, question):\n",
        "        questionTaggers = ['WP','WDT','WP$','WRB']\n",
        "        qPOS = pos_tag(word_tokenize(question))\n",
        "        qTags = []\n",
        "        for token in qPOS:\n",
        "            if token[1] in questionTaggers:\n",
        "                qTags.append(token[1])\n",
        "        qType = ''\n",
        "        if(len(qTags)>1):\n",
        "            qType = 'complex'\n",
        "        elif(len(qTags) == 1):\n",
        "            qType = qTags[0]\n",
        "        else:\n",
        "            qType = \"None\"\n",
        "        return qType\n",
        "    \n",
        "    # To determine type of expected answer depending of question type\n",
        "    #\n",
        "    # Input:\n",
        "    #           question(str) : Question string\n",
        "    # Output:\n",
        "    #           aType(str) : Type of answer among following\n",
        "    #               [PERSON, LOCATION, DATE, ORGANIZATION, QUANTITY, DEFINITION\n",
        "    #                   FULL]\n",
        "    def determineAnswerType(self, question):\n",
        "        questionTaggers = ['WP','WDT','WP$','WRB']\n",
        "        qPOS = pos_tag(word_tokenize(question))\n",
        "        qTag = None\n",
        "\n",
        "        for token in qPOS:\n",
        "            if token[1] in questionTaggers:\n",
        "                qTag = token[0].lower()\n",
        "                break\n",
        "        \n",
        "        if(qTag == None):\n",
        "            if len(qPOS) > 1:\n",
        "                if qPOS[1][1].lower() in ['is','are','can','should']:\n",
        "                    qTag = \"YESNO\"\n",
        "        #who/where/what/why/when/is/are/can/should\n",
        "        if qTag == \"who\":\n",
        "            return \"PERSON\"\n",
        "        elif qTag == \"where\":\n",
        "            return \"LOCATION\"\n",
        "        elif qTag == \"when\":\n",
        "            return \"DATE\"\n",
        "        elif qTag == \"what\":\n",
        "            # Defination type question\n",
        "            # If question of type whd modal noun? its a defination question\n",
        "            qTok = self.getContinuousChunk(question)\n",
        "            #print(qTok)\n",
        "            if(len(qTok) == 4):\n",
        "                if qTok[1][1] in ['is','are','was','were'] and qTok[2][0] in [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]:\n",
        "                    self.question = \" \".join([qTok[0][1],qTok[2][1],qTok[1][1]])\n",
        "                    #print(\"Type of question\",\"Definition\",self.question)\n",
        "                    return \"DEFINITION\"\n",
        "\n",
        "            # ELSE USE FIRST HEAD WORD\n",
        "            for token in qPOS:\n",
        "                if token[0].lower() in [\"city\",\"place\",\"country\"]:\n",
        "                    return \"LOCATION\"\n",
        "                elif token[0].lower() in [\"company\",\"industry\",\"organization\"]:\n",
        "                    return \"ORGANIZATION\"\n",
        "                elif token[1] in [\"NN\",\"NNS\"]:\n",
        "                    return \"FULL\"\n",
        "                elif token[1] in [\"NNP\",\"NNPS\"]:\n",
        "                    return \"FULL\"\n",
        "            return \"FULL\"\n",
        "        elif qTag == \"how\":\n",
        "            if len(qPOS)>1:\n",
        "                t2 = qPOS[2]\n",
        "                if t2[0].lower() in [\"few\",\"great\",\"little\",\"many\",\"much\"]:\n",
        "                    return \"QUANTITY\"\n",
        "                elif t2[0].lower() in [\"tall\",\"wide\",\"big\",\"far\"]:\n",
        "                    return \"LINEAR_MEASURE\"\n",
        "            return \"FULL\"\n",
        "        else:\n",
        "            return \"FULL\"\n",
        "    \n",
        "    # To build search query by dropping question word\n",
        "    #\n",
        "    # Input:\n",
        "    #           question(str) : Question string\n",
        "    # Output:\n",
        "    #           searchQuery(list) : List of tokens\n",
        "    def buildSearchQuery(self, question):\n",
        "        qPOS = pos_tag(word_tokenize(question))\n",
        "        searchQuery = []\n",
        "        questionTaggers = ['WP','WDT','WP$','WRB']\n",
        "        for tag in qPOS:\n",
        "            if tag[1] in questionTaggers:\n",
        "                continue\n",
        "            else:\n",
        "                searchQuery.append(tag[0])\n",
        "                if(self.useSynonyms):\n",
        "                    syn = self.getSynonyms(tag[0])\n",
        "                    if(len(syn) > 0):\n",
        "                        searchQuery.extend(syn)\n",
        "        return searchQuery\n",
        "    \n",
        "    # To build query vector\n",
        "    #\n",
        "    # Input:\n",
        "    #       searchQuery(list) : List of tokens from buildSearchQuery method\n",
        "    # Output:\n",
        "    #       qVector(list) : TFIDF Vectors\n",
        "    def getQueryVector(self, searchQuery):\n",
        "        if self.removeStopwords:\n",
        "            tokens = [w for w in searchQuery if w not in self.stopWords] \n",
        "        else:\n",
        "            tokens = searchQuery\n",
        "\n",
        "        stem = [self.stem(w) for w in tokens] \n",
        "\n",
        "        query_vec = dictionary.doc2bow(stem)\n",
        "        query_vec_tfidf = tfidf[query_vec]\n",
        "\n",
        "        return query_vec_tfidf\n",
        "\n",
        "    \n",
        "    # To get continuous chunk of similar POS tags.\n",
        "    # E.g.  If two NN tags are consequetive, this method will merge and return\n",
        "    #       single NN with combined value.\n",
        "    #       It is helpful in detecting name of single person like John Cena, \n",
        "    #       Steve Jobs\n",
        "    # Input:\n",
        "    #       question(str) : question string\n",
        "    # Output:\n",
        "    #       \n",
        "    def getContinuousChunk(self,question):\n",
        "        chunks = []\n",
        "        answerToken = word_tokenize(question)\n",
        "        nc = pos_tag(answerToken)\n",
        "\n",
        "        prevPos = nc[0][1]\n",
        "        entity = {\"pos\":prevPos,\"chunk\":[]}\n",
        "        for c_node in nc:\n",
        "            (token,pos) = c_node\n",
        "            if pos == prevPos:\n",
        "                prevPos = pos       \n",
        "                entity[\"chunk\"].append(token)\n",
        "            elif prevPos in [\"DT\",\"JJ\"]:\n",
        "                prevPos = pos\n",
        "                entity[\"pos\"] = pos\n",
        "                entity[\"chunk\"].append(token)\n",
        "            else:\n",
        "                if not len(entity[\"chunk\"]) == 0:\n",
        "                    chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "                    entity = {\"pos\":pos,\"chunk\":[token]}\n",
        "                    prevPos = pos\n",
        "        if not len(entity[\"chunk\"]) == 0:\n",
        "            chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "        return chunks\n",
        "    \n",
        "    # To get synonyms of word in order to improve query by using query\n",
        "    # expanision technique\n",
        "    # Input:\n",
        "    #       word(str) : Word token\n",
        "    # Output:\n",
        "    #       synonyms(list) : List of synonyms of given word\n",
        "    def getSynonyms(word):\n",
        "        synonyms = []\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for l in syn.lemmas():\n",
        "                w = l.name().lower()\n",
        "                synonyms.extend(w.split(\"_\"))\n",
        "        return list(set(synonyms))\n",
        "    \n",
        "    # String representation of this class\n",
        "    def __repr__(self):\n",
        "        msg = \"Q: \" + self.question + \"\\n\"\n",
        "        msg += \"QType: \" + self.qType + \"\\n\"\n",
        "        msg += \"QVector: \" + str(self.qVector) + \"\\n\"\n",
        "        return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8s8ZpC1N_tO",
        "colab_type": "code",
        "outputId": "c3983ac3-aa6a-41e4-a1c7-c621708b3ffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "ProcessedQuestion('what are you doing?')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Q: what are you doing?\n",
              "QType: WP\n",
              "QVector: [(75258, 0.6326920128936945), (127339, 0.44654410044367454), (738836, 0.6326920128936945)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAqnIS2kKZNj",
        "colab_type": "text"
      },
      "source": [
        "##DocumentRetrievalModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIOgSn00KXWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ScriptName : DocumentRetrievalModel.py\n",
        "# Description : Script preprocesses article and paragraph to computer TFIDF.\n",
        "#               Additionally, helps in answer processing \n",
        "# Arguments : \n",
        "#       Input :\n",
        "#           useStemmer(boolean)     : Indicate to use stemmer for word tokens\n",
        "#           removeStopWord(boolean) : Indicate to remove stop words from \n",
        "#                                     paragraph in order to keep relevant words\n",
        "#       Output :\n",
        "#           Instance of DocumentRetrievalModel with following structure\n",
        "#               query(function) : Take instance of processedQuestion and return\n",
        "#                                 answer based on IR and Answer Processing\n",
        "#                                 techniques\n",
        "\n",
        "class DocumentRetrievalModel:\n",
        "    def __init__(self, removeStopWord = False,useStemmer = False):\n",
        "        self.stopwords = stopwords.words('english')\n",
        "        self.removeStopWord = removeStopWord\n",
        "        self.useStemmer = useStemmer\n",
        "        self.vData = None\n",
        "        self.stem = lambda k:k.lower()\n",
        "        if(useStemmer):\n",
        "            ps = PorterStemmer()\n",
        "            self.stem = ps.stem\n",
        "            \n",
        "    # To find answer to the question by first finding relevant paragraph, then\n",
        "    # by finding relevant sentence and then by procssing sentence to get answer\n",
        "    # based on expected answer type\n",
        "    # Input:\n",
        "    #           pQ(ProcessedQuestion) : Instance of ProcessedQuestion\n",
        "    # Output:\n",
        "    #           answer(str) : Response of QA System\n",
        "    def query(self,pQ):\n",
        "        \n",
        "        # Get relevant Paragraph\n",
        "        relevantParagraph = self.getSimilarParagraph(pQ.qVector)\n",
        "\n",
        "        # Get All sentences\n",
        "        sentences = []\n",
        "        relevant_para = []\n",
        "        for tup in relevantParagraph:\n",
        "            if tup != None:\n",
        "                p2 = docs[tup[0]]\n",
        "                p2 = \" \".join(p2)\n",
        "                relevant_para.append(p2)\n",
        "                sentences.extend(sent_tokenize(p2))\n",
        "        \n",
        "        # Get Relevant Sentences\n",
        "        if len(sentences) == 0:\n",
        "            return \"Oops! Unable to find answer\"\n",
        "\n",
        "        # Get most relevant sentence using unigram similarity\n",
        "        relevantSentences = self.getMostRelevantSentences(sentences,pQ,1)\n",
        "\n",
        "        # AnswerType\n",
        "        aType = pQ.aType\n",
        "        \n",
        "        # Default Answer\n",
        "        answer = relevantSentences[0][0]\n",
        "        \n",
        "\n",
        "        ps = PorterStemmer()\n",
        "        # For question type looking for Person\n",
        "        if aType == \"PERSON\":\n",
        "            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if entity[0] == \"PERSON\":\n",
        "                    answer = entity[1]\n",
        "                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                    # If any entity is already in question\n",
        "                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                        continue\n",
        "                    break\n",
        "        elif aType == \"LOCATION\":\n",
        "            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if entity[0] == \"GPE\":\n",
        "                    answer = entity[1]\n",
        "                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                    # If any entity is already in question\n",
        "                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                        continue\n",
        "                    break\n",
        "        elif aType == \"ORGANIZATION\":\n",
        "            ne = self.getNamedEntity([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if entity[0] == \"ORGANIZATION\":\n",
        "                    answer = entity[1]\n",
        "                    answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                    # If any entity is already in question\n",
        "                    qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                    if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                        continue\n",
        "                    break\n",
        "        elif aType == \"DATE\":\n",
        "            allDates = []\n",
        "            for s in relevantSentences:\n",
        "                allDates.extend(extractDate(s[0]))\n",
        "            if len(allDates)>0:\n",
        "                answer = allDates[0]\n",
        "        elif aType in [\"NN\",\"NNP\"]:\n",
        "            candidateAnswers = []\n",
        "            ne = self.getContinuousChunk([s[0] for s in relevantSentences])\n",
        "            for entity in ne:\n",
        "                if aType == \"NN\":\n",
        "                    if entity[0] == \"NN\" or entity[0] == \"NNS\":\n",
        "                        answer = entity[1]\n",
        "                        answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                        qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                        # If any entity is already in question\n",
        "                        if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                            continue\n",
        "                        break\n",
        "                elif aType == \"NNP\":\n",
        "                    if entity[0] == \"NNP\" or entity[0] == \"NNPS\":\n",
        "                        answer = entity[1]\n",
        "                        answerTokens = [ps.stem(w) for w in word_tokenize(answer.lower())]\n",
        "                        qTokens = [ps.stem(w) for w in word_tokenize(pQ.question.lower())]\n",
        "                        # If any entity is already in question\n",
        "                        if [(a in qTokens) for a in answerTokens].count(True) >= 1:\n",
        "                            continue\n",
        "                        break\n",
        "        elif aType == \"DEFINITION\":\n",
        "            relevantSentences = self.getMostRelevantSentences(sentences,pQ,1)\n",
        "            answer = relevantSentences[0][0]\n",
        "        \n",
        "        # print('relevant doc:', answer)\n",
        "        return answer, relevant_para\n",
        "        \n",
        "    # Get top 3 relevant paragraph based on cosine similarity between question \n",
        "    # vector and paragraph vector\n",
        "    # Input :\n",
        "    #       queryVector(list) : TFIDF Vector\n",
        "    #\n",
        "    # Output:\n",
        "    #       pRanking(list) : List of tuple with top 3 paragraph with its\n",
        "    #                        similarity coefficient\n",
        "    def getSimilarParagraph(self,queryVector):    \n",
        "        q1_sims = docs_index[queryVector]\n",
        "        q1_sorted_sims = sorted(enumerate(q1_sims), key = lambda item: -item[1])\n",
        "\n",
        "        return q1_sorted_sims[0:5]\n",
        "    \n",
        "    \n",
        "    # Get most relevant sentences using unigram similarity between question\n",
        "    # sentence and sentence in paragraph containing potential answer\n",
        "    # Input:\n",
        "    #       sentences(list)      : List of sentences in order of occurance as in\n",
        "    #                              paragraph\n",
        "    #       pQ(ProcessedQuestion): Instance of processedQuestion\n",
        "    #       nGram(int)           : Value of nGram (default 3)\n",
        "    # Output:\n",
        "    #       relevantSentences(list) : List of tuple with sentence and their\n",
        "    #                                 similarity coefficient\n",
        "    def getMostRelevantSentences(self, sentences, pQ, nGram=3):\n",
        "        relevantSentences = []\n",
        "        for sent in sentences:\n",
        "            sim = 0\n",
        "            sim = self.sim_ngram_sentence(pQ.question,sent,nGram)\n",
        "            relevantSentences.append((sent,sim))\n",
        "        \n",
        "        return sorted(relevantSentences,key=lambda tup:(tup[1],tup[0]),reverse=True)\n",
        "    \n",
        "    # Compute ngram similarity between a sentence and question\n",
        "    # Input:\n",
        "    #       question(str)   : Question string\n",
        "    #       sentence(str)   : Sentence string\n",
        "    #       nGram(int)      : Value of n in nGram\n",
        "    # Output:\n",
        "    #       sim(float)      : Ngram Similarity Coefficient\n",
        "    def sim_ngram_sentence(self, question, sentence,nGram):\n",
        "        #considering stop words as well\n",
        "        ps = PorterStemmer()\n",
        "        getToken = lambda question:[ ps.stem(w.lower()) for w in word_tokenize(question) ]\n",
        "        getNGram = lambda tokens,n:[ \" \".join([tokens[index+i] for i in range(0,n)]) for index in range(0,len(tokens)-n+1)]\n",
        "        qToken = getToken(question)\n",
        "        sToken = getToken(sentence)\n",
        "\n",
        "        if(len(qToken) > nGram):\n",
        "            q3gram = set(getNGram(qToken,nGram))\n",
        "            s3gram = set(getNGram(sToken,nGram))\n",
        "            if(len(s3gram) < nGram):\n",
        "                return 0\n",
        "            qLen = len(q3gram)\n",
        "            sLen = len(s3gram)\n",
        "            sim = len(q3gram.intersection(s3gram)) / len(q3gram.union(s3gram))\n",
        "            return sim\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "    \n",
        "    # Get Named Entity from the sentence in form of PERSON, GPE, & ORGANIZATION\n",
        "    # Input:\n",
        "    #       answers(list)       : List of potential sentence containing answer\n",
        "    # Output:\n",
        "    #       chunks(list)        : List of tuple with entity and name in ranked \n",
        "    #                             order\n",
        "    def getNamedEntity(self,answers):\n",
        "        chunks = []\n",
        "        for answer in answers:\n",
        "            answerToken = word_tokenize(answer)\n",
        "            nc = ne_chunk(pos_tag(answerToken))\n",
        "            entity = {\"label\":None,\"chunk\":[]}\n",
        "            for c_node in nc:\n",
        "                if(type(c_node) == Tree):\n",
        "                    if(entity[\"label\"] == None):\n",
        "                        entity[\"label\"] = c_node.label()\n",
        "                    entity[\"chunk\"].extend([ token for (token,pos) in c_node.leaves()])\n",
        "                else:\n",
        "                    (token,pos) = c_node\n",
        "                    if pos == \"NNP\":\n",
        "                        entity[\"chunk\"].append(token)\n",
        "                    else:\n",
        "                        if not len(entity[\"chunk\"]) == 0:\n",
        "                            chunks.append((entity[\"label\"],\" \".join(entity[\"chunk\"])))\n",
        "                            entity = {\"label\":None,\"chunk\":[]}\n",
        "            if not len(entity[\"chunk\"]) == 0:\n",
        "                chunks.append((entity[\"label\"],\" \".join(entity[\"chunk\"])))\n",
        "        return chunks\n",
        "    \n",
        "    # To get continuous chunk of similar POS tags.\n",
        "    # E.g.  If two NN tags are consequetive, this method will merge and return\n",
        "    #       single NN with combined value.\n",
        "    #       It is helpful in detecting name of single person like John Cena, \n",
        "    #       Steve Jobs\n",
        "    # Input:\n",
        "    #       answers(list) : list of potential sentence string\n",
        "    # Output:\n",
        "    #       chunks(list)  : list of tuple with entity and name in ranked order\n",
        "    def getContinuousChunk(self,answers):\n",
        "        chunks = []\n",
        "        for answer in answers:\n",
        "            answerToken = word_tokenize(answer)\n",
        "            if(len(answerToken)==0):\n",
        "                continue\n",
        "            nc = pos_tag(answerToken)\n",
        "            \n",
        "            prevPos = nc[0][1]\n",
        "            entity = {\"pos\":prevPos,\"chunk\":[]}\n",
        "            for c_node in nc:\n",
        "                (token,pos) = c_node\n",
        "                if pos == prevPos:\n",
        "                    prevPos = pos       \n",
        "                    entity[\"chunk\"].append(token)\n",
        "                elif prevPos in [\"DT\",\"JJ\"]:\n",
        "                    prevPos = pos\n",
        "                    entity[\"pos\"] = pos\n",
        "                    entity[\"chunk\"].append(token)\n",
        "                else:\n",
        "                    if not len(entity[\"chunk\"]) == 0:\n",
        "                        chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "                        entity = {\"pos\":pos,\"chunk\":[token]}\n",
        "                        prevPos = pos\n",
        "            if not len(entity[\"chunk\"]) == 0:\n",
        "                chunks.append((entity[\"pos\"],\" \".join(entity[\"chunk\"])))\n",
        "        return chunks\n",
        "        \n",
        "    # def __repr__(self):\n",
        "    #     msg = \"Total Paras \" + str(self.totalParas) + \"\\n\"\n",
        "    #     msg += \"Total Unique Word \" + str(len(self.idf)) + \"\\n\"\n",
        "    #     return msg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DU6uMR2UGLu",
        "colab_type": "code",
        "outputId": "f4b3b970-20d1-4a23-8528-7079ddb604d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "drm = DocumentRetrievalModel()\n",
        "drm.query(ProcessedQuestion('what are you doing?'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('akc standard hardly altered original 1896 principal change colours black tan ared tan tawny renamed black tan liver tan ared british made considerable changes',\n",
              " ['star trek voyager fictional spacecraft named ares lrbnot confused actual ares rocket rrbis launched early 2032 trapped inside graviton ellipse huge body subspace energy travelling galaxy',\n",
              "  'one called orange leafrust ared rather orange stage leafblades prevalent conspicuous black stage came later black stem rust could easily mistaken ared uredostage unmistakable extremely destructive black stage stems',\n",
              "  'akc standard hardly altered original 1896 principal change colours black tan ared tan tawny renamed black tan liver tan ared british made considerable changes',\n",
              "  'ares ancient greek mythology god war rather battle son zeus hera roman god identified ares see mars contrasted athena added attributes goddess skilfully conducted military operations personifies brute strength wild rage conflict delight war bloodshed loves fighting fighting sake takes side one combatant indifferently regardless justice cause',\n",
              "  'antediluvian alien drooling dasein dwells primarily primordially amidst ancient subterranean shuddering shimmering sensations shining raw radiant register dreading dasein dazzling doingnesses unsettled unhinged unattached undone withering wondering wombing wandering woozy sensation absolute knowledge'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSr50B3TnLkq",
        "colab_type": "text"
      },
      "source": [
        "##MCQ matching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SEyeuamnKCK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_option(answer, options):\n",
        "    # compute TFIDF vectors for answer and options\n",
        "    ans_tokens = word_tokenize(answer.lower())\n",
        "    ans_stop = [w for w in ans_tokens if w not in stop_list] \n",
        "    ans_stem = [stemmer.stem(w) for w in ans_stop] \n",
        "\n",
        "    ans_vec = dictionary.doc2bow(ans_stem)\n",
        "    ans_vec_tfidf = tfidf[ans_vec]\n",
        "\n",
        "    opt_tokens = [word_tokenize(opt.lower()) for opt in options]\n",
        "    stop_tokens = [[w for w in opt if w not in stop_list] for opt in opt_tokens]\n",
        "    stem_tokens = [[stemmer.stem(w) for w in opt] for opt in stop_tokens]\n",
        "\n",
        "    opt_vecs = [dictionary.doc2bow(opt) for opt in stem_tokens]\n",
        "    opt_vecs_tfidf = [tfidf[vec] for vec in opt_vecs]\n",
        "\n",
        "    # compute similarity \n",
        "    mcq_index = similarities.SparseMatrixSimilarity(opt_vecs_tfidf, n_dict)\n",
        "    sims = mcq_index[ans_vec_tfidf]\n",
        "    sorted_sims = sorted(enumerate(sims), key = lambda item: -item[1])\n",
        "\n",
        "    return sorted_sims\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3WckUkDVK2w",
        "colab_type": "text"
      },
      "source": [
        "##MCQ matching by computing similarity with Doc2vec (built from 0.1% corpus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnjJbgiDLJzD",
        "colab_type": "code",
        "outputId": "1b629d13-f780-4243-8db8-fb083cb2781f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# from gensim.models.doc2vec import Doc2Vec\n",
        "# model_path = '/content/drive/My Drive/TM self/doc2vec0.1/d2v.model'\n",
        "# model= Doc2Vec.load(model_path)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e1M_MtaV43S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from scipy import spatial\n",
        "# # return list of tuple of option index and similarity score\n",
        "\n",
        "# def get_option(response, options):\n",
        "#     opt_vecs = []\n",
        "#     r_vec = model.infer_vector(word_tokenize(response.lower()))\n",
        "#     for opt in options:\n",
        "#         a = model.infer_vector(word_tokenize(opt.lower()))\n",
        "#         opt_vecs.append(a)\n",
        "\n",
        "#     answers = []\n",
        "#     for i, v in enumerate(opt_vecs):\n",
        "#         result = 1 - spatial.distance.cosine(r_vec, v)\n",
        "#         answers.append((i, result))\n",
        "\n",
        "#     return sorted(answers, key=lambda tup: (tup[1],tup[0]), reverse=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCssVVMJcI8C",
        "colab_type": "text"
      },
      "source": [
        "##Test 1 query"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bhTKYNXbgbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drm = DocumentRetrievalModel(True,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uK9VpkxY-Gp",
        "colab_type": "code",
        "outputId": "5de1c7df-70d1-4ee8-e7a1-a73c6fc14a9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# userQuery = 'Which of the following is a typical example of a unicellular organism? (A) earthworm (B) bacteria (C) fungi (D) green algae'\n",
        "userQuery = \"What is the role of decomposers in a food chain? (A) They consume other organisms. (B) They break down dead organic matter. (C) They use the Sun's energy to make food. (D) They convert inorganic matter into organic matter.\"\n",
        "output = re.split(\"\\(\\w+\\)\", userQuery)\n",
        "\n",
        "question = output[0]\n",
        "options = output[1:]\n",
        "\n",
        "    # Proocess Question\n",
        "pq = ProcessedQuestion(question,True,False,True) #qns, stem, syn, stopw removal \n",
        "\n",
        "    # Get Response From Bot\n",
        "answer, relevant_docs = drm.query(pq)\n",
        "\n",
        "answers_sim = get_option(answer, options)\n",
        "# response = answers_sim[0][0] #first option\n",
        "answers_sim"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 0.23944664), (0, 0.12476348), (1, 0.0), (3, 0.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnccgwnNpmZ4",
        "colab_type": "code",
        "outputId": "7c09bd3d-1315-4d1e-c077-c39915a87d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "relevant_docs"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['food chain food chain defined process transfer energy one trophic level another process eating eatenthe different members trophic level either act predators consumers finally decomposers set decompose organic remains return elements naturethere different types food chains like grazing food chain detritus food chain parasitic food chains aquatic food chains etc',\n",
              " 'completed accurate food chain class label component food chain decomposer producer consumer etc finished explain students going make food chain using website create food chain',\n",
              " 'food chain consumer producer decomposer sun together food chain art project additionally carnivore food chain furthermore animal food chain kids furthermore cartoon ocean food chain also fish food chain clip art including food chain clip art well fish eating food together food chain clip art photo',\n",
              " 'today clipart food chain offers along pictures also desert food chain including chain survival clip art together chain clip art along tundra biome food chain additionally fish food chain along food chain examples along marine food chain clip art well food chain diagram furthermore funny food chain including food chain illustration',\n",
              " 'powerful links pyramid diagram construct food chain define carnivore herbivore omnivore producer consumer energy pyramid decomposer food chain food web contrast food chains food webs estimate calculate percentages analyze value shorter food chains longer ones eisenhower']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTb5_pT-cNTN",
        "colab_type": "text"
      },
      "source": [
        "##Get Accuracy on ARC Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkUodkPqcMpO",
        "colab_type": "code",
        "outputId": "89a28ecd-14a4-4dff-ca53-f8452cfd5b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "test_path = \"/content/drive/My Drive/TM/Project/data/ARC/raw/ARC-Easy-Test.csv\"\n",
        "qa = pd.read_csv(test_path)\n",
        "qa.head(10)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>schoolGrade</th>\n",
              "      <th>year</th>\n",
              "      <th>question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mercury_417466</td>\n",
              "      <td>417466</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which statement best explains why photosynthes...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mercury_7081673</td>\n",
              "      <td>7081673</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which piece of safety equipment is used to kee...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mercury_7239733</td>\n",
              "      <td>7239733</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>Meiosis is a type of cell division in which ge...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYSEDREGENTS_2015_4_8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>D</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>4</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which characteristic describes the texture of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mercury_7037258</td>\n",
              "      <td>7037258</td>\n",
              "      <td>1</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which best describes the structure of an atom?...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>CSZ20679</td>\n",
              "      <td>CSZ20679</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>California Standards Test</td>\n",
              "      <td>8</td>\n",
              "      <td>2009</td>\n",
              "      <td>To express the distance between the Milky Way ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mercury_182158</td>\n",
              "      <td>182158</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>A student has just completed a laboratory acti...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Mercury_7216668</td>\n",
              "      <td>7216668</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Students are investigating the effects of diff...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MCAS_2001_5_19</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>C</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>MCAS</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>Plants use sunlight to make (A) soil. (B) mine...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Mercury_SC_413631</td>\n",
              "      <td>413631</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which of these correctly identifies the way ma...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              questionID originalQuestionID  ...  subject category\n",
              "0         Mercury_417466             417466  ...      NaN     Test\n",
              "1        Mercury_7081673            7081673  ...      NaN     Test\n",
              "2        Mercury_7239733            7239733  ...      NaN     Test\n",
              "3  NYSEDREGENTS_2015_4_8                  8  ...      NaN     Test\n",
              "4        Mercury_7037258            7037258  ...      NaN     Test\n",
              "5               CSZ20679           CSZ20679  ...      NaN     Test\n",
              "6         Mercury_182158             182158  ...      NaN     Test\n",
              "7        Mercury_7216668            7216668  ...      NaN     Test\n",
              "8         MCAS_2001_5_19                 19  ...      NaN     Test\n",
              "9      Mercury_SC_413631             413631  ...      NaN     Test\n",
              "\n",
              "[10 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge8RBUTHcHg1",
        "colab_type": "code",
        "outputId": "fc3d617e-21a7-4123-a1e9-f39e01e0863c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "print(qa['totalPossiblePoint'].sum())\n",
        "print(qa['isMultipleChoiceQuestion'].sum())\n",
        "print(len(qa))\n",
        "print(qa['AnswerKey'].value_counts())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2376\n",
            "2376\n",
            "2376\n",
            "C    610\n",
            "A    570\n",
            "B    563\n",
            "D    535\n",
            "1     26\n",
            "4     26\n",
            "3     23\n",
            "2     22\n",
            "E      1\n",
            "Name: AnswerKey, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5j_gD0Adzpw",
        "colab_type": "code",
        "outputId": "31cf62b8-55af-4df6-c673-6e7ad4b504e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "ans_map = {'A':0, 'B':1, 'C':2, 'D':3, 'E':4, '1':0, '2':1, '3':2, '4':3}\n",
        "\n",
        "qa = qa.replace({\"AnswerKey\": ans_map})\n",
        "qa['AnswerKey'] = qa['AnswerKey'].astype('int')\n",
        "qa.head()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questionID</th>\n",
              "      <th>originalQuestionID</th>\n",
              "      <th>totalPossiblePoint</th>\n",
              "      <th>AnswerKey</th>\n",
              "      <th>isMultipleChoiceQuestion</th>\n",
              "      <th>includesDiagram</th>\n",
              "      <th>examName</th>\n",
              "      <th>schoolGrade</th>\n",
              "      <th>year</th>\n",
              "      <th>question</th>\n",
              "      <th>subject</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mercury_417466</td>\n",
              "      <td>417466</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which statement best explains why photosynthes...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mercury_7081673</td>\n",
              "      <td>7081673</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which piece of safety equipment is used to kee...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mercury_7239733</td>\n",
              "      <td>7239733</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>Meiosis is a type of cell division in which ge...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NYSEDREGENTS_2015_4_8</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NYSEDREGENTS</td>\n",
              "      <td>4</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which characteristic describes the texture of ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mercury_7037258</td>\n",
              "      <td>7037258</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Mercury</td>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>Which best describes the structure of an atom?...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              questionID originalQuestionID  ...  subject  category\n",
              "0         Mercury_417466             417466  ...      NaN      Test\n",
              "1        Mercury_7081673            7081673  ...      NaN      Test\n",
              "2        Mercury_7239733            7239733  ...      NaN      Test\n",
              "3  NYSEDREGENTS_2015_4_8                  8  ...      NaN      Test\n",
              "4        Mercury_7037258            7037258  ...      NaN      Test\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o6jAcFGe-c4",
        "colab_type": "code",
        "outputId": "2382bdc5-b390-47ce-d4e8-96459649ec17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "print(qa['AnswerKey'].value_counts())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2    633\n",
            "0    596\n",
            "1    585\n",
            "3    561\n",
            "4      1\n",
            "Name: AnswerKey, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AowiYosCfZ16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# separate qns and options for input\n",
        "# qns_list = [re.split(\"\\(\\w+\\)\", row)[0] for row in qa['question']]\n",
        "# options_list =  [re.split(\"\\(\\w+\\)\", row)[1:] for row in qa['question']]\n",
        "\n",
        "# qns and options together for input\n",
        "qns_list = [\" \".join(re.split(\"\\(\\w+\\)\", row)) for row in qa['question']]\n",
        "options_list =  [re.split(\"\\(\\w+\\)\", row)[1:] for row in qa['question']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rOPgOBFRlIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c94115fa-76f9-4d8b-8416-6ed8e2248e2d"
      },
      "source": [
        "qns_list[0]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Which statement best explains why photosynthesis is the foundation of most food webs?   Sunlight is the source of energy for nearly all ecosystems.   Most ecosystems are found on land instead of in water.   Carbon dioxide is more available than other gases.   The producers in all ecosystems are plants.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXUcksbqX6UP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "01a19384-e5e2-4f69-8b47-82b815b37d2e"
      },
      "source": [
        "options_list[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Sunlight is the source of energy for nearly all ecosystems. ',\n",
              " ' Most ecosystems are found on land instead of in water. ',\n",
              " ' Carbon dioxide is more available than other gases. ',\n",
              " ' The producers in all ecosystems are plants.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPzV8njigTkW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_ans = []\n",
        "for i in range(len(qns_list)):\n",
        "    qns = qns_list[i]\n",
        "    mcq = options_list[i]\n",
        "    pq = ProcessedQuestion(qns,True,False,True)\n",
        "    answer, rel_docs = drm.query(pq)\n",
        "    answers_sim = get_option(answer, mcq)\n",
        "    response = answers_sim[0][0] #first option\n",
        "    predicted_ans.append(response)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPaavNsQgTuI",
        "colab_type": "code",
        "outputId": "9a989e27-d048-4748-fc6e-fe17f4162d82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# separate qns and options for input\n",
        "from sklearn.metrics import accuracy_score\n",
        "true_ans = qa['AnswerKey'].tolist()\n",
        "accuracy_score(true_ans, predicted_ans)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3202861952861953"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzChpYFoRtvk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a9f831c-445a-4a03-c680-7f7f14033db9"
      },
      "source": [
        "# qns and options together for input\n",
        "from sklearn.metrics import accuracy_score\n",
        "true_ans = qa['AnswerKey'].tolist()\n",
        "accuracy_score(true_ans, predicted_ans)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36784511784511786"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMu0oTQjK8Pm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"Bot> Please wait, while I am loading my dependencies\")\n",
        "# # from DocumentRetrievalModel import DocumentRetrievalModel as DRM\n",
        "# # from ProcessedQuestion import ProcessedQuestion as PQ\n",
        "# import re\n",
        "# import sys\n",
        "\n",
        "# # if len(sys.argv) == 1:\n",
        "# # \tprint(\"Bot> I need some reference to answer your question\")\n",
        "# # \tprint(\"Bot> Please! Rerun me using following syntax\")\n",
        "# # \tprint(\"\\t\\t$ python3 P2.py <datasetName>\")\n",
        "# # \tprint(\"Bot> You can find dataset name in \\\"dataset\\\" folder\")\n",
        "# # \tprint(\"Bot> Thanks! Bye\")\n",
        "# # \texit()\n",
        "\n",
        "# # datasetName = sys.argv[1]\n",
        "# datasetName = '/content/drive/My Drive/TM self/Factoid-based-Question-Answer-Chatbot/testing3.txt' #change here to corpus txt with \n",
        "# # Loading Dataset\n",
        "# try:\n",
        "# \tdatasetFile = open(datasetName,\"r\")\n",
        "# except FileNotFoundError:\n",
        "# \tprint(\"Bot> Oops! I am unable to locate \\\"\" + datasetName + \"\\\"\")\n",
        "# \texit()\n",
        "\n",
        "# # Retrieving paragraphs : Assumption is that each paragraph in dataset is\n",
        "# # separated by new line character\n",
        "# paragraphs = []\n",
        "# for para in datasetFile.readlines():\n",
        "# \tif(len(para.strip()) > 0):\n",
        "# \t\tparagraphs.append(para.strip())\n",
        "\n",
        "# # Processing Paragraphs\n",
        "# drm = DocumentRetrievalModel(paragraphs,True,True) #docs, stem, stopw removal\n",
        "\n",
        "# print(\"Bot> Hey! I am ready. Ask me factoid based questions only :P\")\n",
        "# print(\"Bot> You can say Bye anytime you want\")\n",
        "\n",
        "# # Greet Pattern\n",
        "# greetPattern = re.compile(\"^\\ *((hi+)|((good\\ )?morning|evening|afternoon)|(he((llo)|y+)))\\ *$\",re.IGNORECASE)\n",
        "\n",
        "# isActive = True\n",
        "# while isActive:\n",
        "#   userQuery = input(\"You> \")\n",
        "#   if not len(userQuery)>0:\n",
        "#     print(\"Bot> You need to ask something\")\n",
        "#   elif greetPattern.findall(userQuery):\n",
        "#     response = \"Hello!\"\n",
        "#   elif (userQuery.strip().lower() == \"bye\"):\n",
        "#     response = \"Bye Bye!\"\n",
        "#     isActive = False\n",
        "#   else:\n",
        "#     # split input into list of qns and options\n",
        "#     output = re.split(\"\\(\\w+\\)\", userQuery)\n",
        "\n",
        "#     question = output[0]\n",
        "#     options = output[1:]\n",
        "\n",
        "# \t\t# Proocess Question\n",
        "#     pq = ProcessedQuestion(question,True,False,True) #qns, stem, syn, stopw removal \n",
        "\n",
        "# \t\t# Get Response From Bot\n",
        "#     answer = drm.query(pq)\n",
        "\n",
        "#     # Get option by comparing with response\n",
        "#     # response = get_option(answer, options)\n",
        "#     answers_sim = get_option(answer, options)\n",
        "#     response = answers_sim[0][0] #first option\n",
        "\n",
        "#   print(\"Bot>\",response)\n",
        "#   # print('Related docs>', answers_sim)\n",
        "#   print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jbFyOMwT9Mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyqG-ZbATY9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE0AQC5_TbBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuBRyQu8ZQ8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsqCyojTbIlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls9pmFPPbdME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4kZGJ_7bdF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myV_gYkgTvTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH6wvjpcTvOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM6YYVu9TDk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Which of the following is a typical example of a unicellular organism? (A) earthworm (B) bacteria (C) fungi (D) green algae\n",
        "B\n",
        "\n",
        "What role does the centromere play in cellular reproduction? (A) It is the area where microtubules are formed. (B) It is the area where the nucleus is during cell division. (C) It is the area of alignment for the chromosomes. (D) It is the area of attachment for chromatids.\n",
        "D\n",
        "\n",
        "What causes a blue block to appear blue in the sunlight? (A) The block absorbs all blue light. (B) The block bends (refracts) all blue light. (C) Only blue light is reflected by the block. (D) Only blue light passes through the block.\n",
        "C\n",
        "\n",
        "To safely conduct an experiment using chemicals, what should students always do? (A) Work in large groups. (B) Wear safety goggles. (C) Wear short sleeves. (D) Keep a window open.\n",
        "B\n",
        "\n",
        "Which are two parts of the carbon cycle? (A) freezing and thawing (B) growth and reproduction (C) evaporation and precipitation (D) photosynthesis and respiration\n",
        "D\n",
        "\n",
        "The digestive system breaks food into simple substances that the body can use. What system carries these simple substances from the digestive system to other parts of the body? (A) circulatory (B) nervous (C) respiratory (D) skeletal\n",
        "A\n",
        "\n",
        "What tool would be used to examine a fingerprint? (A) a graduated cylinder (B) a hand lens (C) a pair of goggles (D) a thermometer\n",
        "B\n",
        "\n",
        "Which sense is used to tell if there is sugar in a glass of tea? (A) Touch (B) Hearing (C) Smell (D) Taste\n",
        "D\n",
        "\n",
        "All organisms contain DNA and RNA. What are the subunits of DNA and RNA? (A) simple sugars (B) amino acids (C) carbohydrates (D) nucleotides\n",
        "D\n",
        "\n",
        "What happens during photosynthesis? (A) Insects pollinate plants. (B) Plants change soil into food energy. (C) Animals get carbon dioxide from plants. (D) Plants change light energy into food energy.\n",
        "D\n",
        "\n",
        "What is the role of decomposers in a food chain? (A) They consume other organisms. (B) They break down dead organic matter. (C) They use the Sun's energy to make food. (D) They convert inorganic matter into organic matter.\n",
        "B\n",
        "\n",
        "Which part of the electromagnetic spectrum can humans sense without using equipment or technology? (A) radio waves (B) visible light (C) microwaves (D) X-rays\n",
        "B\n",
        "\n",
        "What does a mirror do to light that causes objects to appear backwards? (A) refracts (B) reflects (C) absorbs (D) blocks\n",
        "B"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
